import numpy as np

# goal: get some new random text out of it as a prediction for char-char network
text = """Hello, everyone! This is the LONGEST TEXT EVER! I was inspired by the various other "longest texts ever" on the internet, and I wanted to make my own. So here it is! This is going to be a WORLD RECORD! This is actually my third attempt at doing this. The first time, I didn't save it. The second time, the Neocities editor crashed. Now I'm writing this in Notepad, then copying it into the Neocities editor instead of typing it directly in the Neocities editor to avoid crashing. It sucks that my past two attempts are gone now. Those actually got pretty long. Not the longest, but still pretty long. I hope this one won't get lost somehow. Anyways, let's talk about WAFFLES! I like waffles. Waffles are cool. Waffles is a funny word. There's a Teen Titans Go episode called "Waffles" where the word "Waffles" is said a hundred-something times. It's pretty annoying. There's also a Teen Titans Go episode about Pig Latin. Don't know what Pig Latin is? It's a language where you take all the consonants before the first vowel, move them to the end, and add '-ay' to the end. If the word begins with a vowel, you just add '-way' to the end. For example, "Waffles" becomes "Afflesway". I've been speaking Pig Latin fluently since the fourth grade, so it surprised me when I saw the episode for the first time. I speak Pig Latin with my sister sometimes. It's pretty fun. I like speaking it in public so that everyone around us gets confused. That's never actually happened before, but if it ever does, 'twill be pretty funny. By the way, "'twill" is a word I invented recently, and it's a contraction of "it will". I really hope it gains popularity in the near future, because "'twill" is WAY more fun than saying "it'll". "It'll" is too boring. Nobody likes boring. This is nowhere near being the longest text ever, but eventually it will be! I might still be writing this a decade later, who knows? But right now, it's not very long. But I'll just keep writing until it is the longest! Have you ever heard the song "Dau Dau" by Awesome Scampis? It's an amazing song. Look it up on YouTube! I play that song all the time around my sister! It drives her crazy, and I love it. Another way I like driving my sister crazy is by speaking my own made up language to her. She hates the languages I make! The only language that we both speak besides English is Pig Latin. I think you already knew that. Whatever. I think I'm gonna go for now. Bye! Hi, I'm back now. I'm gonna contribute more to this soon-to-be giant wall of text. I just realised I have a giant stuffed frog on my bed. I forgot his name. I'm pretty sure it was something stupid though. I think it was "FROG" in Morse Code or something. Morse Code is cool. I know a bit of it, but I'm not very good at it. I'm also not very good at French. I barely know anything in French, and my pronunciation probably sucks. But I'm learning it, at least. I'm also learning Esperanto. It's this language that was made up by some guy a long time ago to be the "universal language". A lot of people speak it. I am such a language nerd. Half of this text is probably gonna be about languages. But hey, as long as it's long! Ha, get it? As LONG as it's LONG? I'm so funny, right? No, I'm not. I should probably get some sleep. Goodnight! Hello, I'm back again. I basically have only two interests nowadays: languages and furries. What? Oh, sorry, I thought you knew I was a furry. Haha, oops. Anyway, yeah, I'm a furry, but since I'm a young furry, I can't really do as much as I would like to do in the fandom. When I'm older, I would like to have a fursuit, go to furry conventions, all that stuff. But for now I can only dream of that. Sorry you had to deal with me talking about furries, but I'm honestly very desperate for this to be the longest text ever. Last night I was watching nothing but fursuit unboxings. I think I need help. This one time, me and my mom were going to go to a furry Christmas party, but we didn't end up going because of the fact that there was alcohol on the premises, and that she didn't wanna have to be a mom dragging her son through a crowd of furries. Both of those reasons were understandable. Okay, hopefully I won't have to talk about furries anymore. I don't care if you're a furry reading this right now, I just don't wanna have to torture everyone else. I will no longer say the F word throughout the rest of this entire text. Of course, by the F word, I mean the one that I just used six times, not the one that you're probably thinking of which I have not used throughout this entire text. I just realised that next year will be 2020. That's crazy! It just feels so futuristic! It's also crazy that the 2010s decade is almost over. That decade brought be a lot of memories. In fact, it brought be almost all of my memories. It'll be sad to see it go. I'm gonna work on a series of video lessons for Toki Pona. I'll expain what Toki Pona is after I come back. Bye! I'm back now, and I decided not to do it on Toki Pona, since many other people have done Toki Pona video lessons already. I decided to do it on Viesa, my English code. Now, I shall explain what Toki Pona is. Toki Pona is a minimalist constructed language that has only ~120 words! That means you can learn it very quickly. I reccomend you learn it! It's pretty fun and easy! Anyway, yeah, I might finish my video about Viesa later. But for now, I'm gonna add more to this giant wall of text, because I want it to be the longest! It would be pretty cool to have a world record for the longest text ever. Not sure how famous I'll get from it, but it'll be cool nonetheless. Nonetheless. That's an interesting word. It's a combination of three entire words. That's pretty neat. Also, remember when I said that I said the F word six times throughout this text? I actually messed up there. I actually said it ten times (including the plural form). I'm such a liar! I struggled to spell the word "liar" there. I tried spelling it "lyer", then "lier". Then I remembered that it's "liar". At least I'm better at spelling than my sister. She's younger than me, so I guess it's understandable. "Understandable" is a pretty long word. Hey, I wonder what the most common word I've used so far in this text is. I checked, and appearantly it's "I", with 59 uses! The word "I" makes up 5% of the words this text! I would've thought "the" would be the most common, but "the" is only the second most used word, with 43 uses. "It" is the third most common, followed by "a" and "to". Congrats to those five words! If you're wondering what the least common word is, well, it's actually a tie between a bunch of words that are only used once, and I don't wanna have to list them all here. Remember when I talked about waffles near the beginning of this text? Well, I just put some waffles in the toaster, and I got reminded of the very beginnings of this longest text ever. Okay, that was literally yesterday, but I don't care. You can't see me right now, but I'm typing with my nose! Okay, I was not able to type the exclamation point with just my nose. I had to use my finger. But still, I typed all of that sentence with my nose! I'm not typing with my nose right now, because it takes too long, and I wanna get this text as long as possible quickly. I'm gonna take a break for now! Bye! Hi, I'm back again. My sister is beside me, watching me write in this endless wall of text. My sister has a new thing where she just says the word "poop" nonstop. I don't really like it. She also eats her own boogers. I'm not joking. She's gross like that. Also, remember when I said I put waffles in the toaster? Well, I forgot about those and I only ate them just now. Now my sister is just saying random numbers. Now she's saying that they're not random, they're the numbers being displayed on the microwave. Still, I don't know why she's doing that. Now she's making annoying clicking noises. Now she's saying that she's gonna watch Friends on three different devices. Why!?!?! Hi its me his sister. I'd like to say that all of that is not true. Max wants to make his own video but i wont let him because i need my phone for my alarm.POOP POOP POOP POOP LOL IM FUNNY. kjnbhhisdnhidfhdfhjsdjksdnjhdfhdfghdfghdfbhdfbcbhnidjsduhchyduhyduhdhcduhduhdcdhcdhjdnjdnhjsdjxnj Hey, I'm back. Sorry about my sister. I had to seize control of the LTE from her because she was doing keymash. Keymash is just effortless. She just went back to school. She comes home from school for her lunch break. I think I'm gonna go again. Bye! Hello, I'm back. Let's compare LTE's. This one is only 8593 characters long so far. Kenneth Iman's LTE is 21425 characters long. The Flaming-Chicken LTE (the original) is a whopping 203941 characters long! I think I'll be able to surpass Kenneth Iman's not long from now. But my goal is to surpass the Flaming-Chicken LTE. Actually, I just figured out that there's an LTE longer than the Flaming-Chicken LTE. It's Hermnerps LTE, which is only slightly longer than the Flaming-Chicken LTE, at 230634 characters. My goal is to surpass THAT. Then I'll be the world record holder, I think. But I'll still be writing this even after I achieve the world record, of course. One time, I printed an entire copy of the Bee Movie script for no reason. I heard someone else say they had three copies of the Bee Movie script in their backpack, and I got inspired. But I only made one copy because I didn't want to waste THAT much paper. I still wasted quite a bit of paper, though. Now I wanna see how this LTE compares to the Bee Movie script. Okay, I checked, and the Bee Movie script is 50753 characters long. Not as long as some of the LTEs I mentioned, but still longer than mine and Kenneth Iman's combined. This LTE is getting close to 10000 characters! That means it'll be half the length of Kenneth Iman's LTE. That's pretty exciting. Also, going back to the topic of the Bee Movie Script, I tried to write the entire thing out by hand once. But I never finished it, especially since I'm focusing on this thing now. Maybe I should write this LTE out by hand. Nah, I don't think I will. Yay, we're at 10000 characters! Let's celebrate by talking about MUSIC! Music is cool. That concludes our celebratory discussion about music. Thank you, and have a good rest of your day. Hi, I'm back now, and I got a book! It's a dictionary for a language called Elefen. It's like Esperanto, but better. Now I can learn Elefen even without internet! That's pretty cool. I will now write something in Elefen. See if you can understand it! Here goes: Si tu pote leje esta, tu es merveliosa! Elefen es un lingua multe fresca! Did you understand that? Maybe you can't speak Elefen, but you still understood that because of your knowledge of other languages. Elefen is cool because it's an actual language, not an English code like Pig Latin or Viesa. Oh, I forgot to mention that my sister is back from school. She's blasting Rhett and Link songs right now. Have you seen that picture of Rhett and Link standing with a bunch of *******? Sorry, I almost said the F word there. That would've broken my rule of not saying the F word. I wrote something in Elefen, so I will also write something in Toki Pona. See if you can understand it now! sina sona e toki mi la sina pona mute a! I can speak Toki Pona fluently, by the way. It's also a pretty cool language. My sister is still playing annoying songs. It's hindering my focus right now. But it's fiiiiine. Okay, luckily she's run out of songs to play. At least for now. She's trying to think of another annoying song to play. Now she's playing a song by Green Day. Not NEARLY as bad as the other songs she just played. I should go for now. Goodbye! Hello, I'm back once again. I don't know why I feel obligated to say that every time I come back. But I'll keep doing it anyway. My sister stopped blasting annoying songs, so that's good. She's cooking something in the microwave. I'll go check to see what it is right now. Nevermind, it's already done cooking. Right, I remember! It's mac and cheese! Now she just started singing "I have a tongue, you don't, because I cut it off yesterday". I don't know what goes on in her mind when she does stuff like that. I've been messing around with my Elefen dictionary for a while, looking up whatever random words I can think of. By the way, the whole reason I'm doing this longest text ever is because of pointlesssites.com. That's how I found the Flaming-Chicken LTE, which inspired me to start writing this LTE. So thanks, pointlesssites.com! I check that website every day to see what new pointless websites they add. You know, I could double every letter I type so that this text would be twice as long as it normally would be. But nah, that's kinda cheating. So I won't. Also, SUBSCRIBE TO PEWDIEPIE! There, I did my part. Not that anyone will read this, but still. 'Twould be nice if you subscribed to PewDiePie. That's another word I invented. Actually, I looked it up, and I didn't invent it. Someone came up with it before I did. That's pretty sad. Also, LEARN VIESA TODAY! IT WILL CURE YOUR DEPRESSION! Seriously though, learn Viesa. It won't actually cure your depression, but I'm desperate for speakers. I only have one other person to speak it with. I should go now. Goodbye. Hi, I’m back. I just came up with an idea: SIMPLIFIED ENGLISH! Or, in Simplified Engish: Simifid Enis. It’s where every group of consonant letters is reduced to the first consonant in that group of consonants, and same goes with the vowels. If a word ends up being just a single consonant with no vowel, put ‘a’ at the end. So “I like eating my waffles” becomes “I like etin ma wafes”. Isn’t it the most amazing thing ever? Nah, it’s not quite as amazing as Viesa. Actually, Viesa isn’t a real language, so it’s less amazing then Elefen and Toki Pona, both of which are cool languages. I kinda figured that half of this text would be about languages. Oh well. I just really want this to be the longest text ever, without using copy and paste, keymash, etc. If you remember, my sister did a little bit of keymash in this text a while ago. I would’ve deleted it, but nah, I didn’t feel like it. And besides, it’s not like it took up half this text. I have an estimate for how long it’ll take me to be the world record holder: about one month. I think I can manage one month of writing this. You know what? I’m just gonna break my rule of not saying the word “furry”. There, I said it. Now I’m allowing myself to write “furry” whenever I want. So with that out of the way, let’s talk about how I first became a furry. For some reason, I have the exact date when I became a furry memorized. It’s May 4, 2018. At that time, I discovered that I was a furry by watching some furry YouTube videos. I knew about the existence of furries years before this, but I didn’t know much about it until this time. I said to myself, “You know what? I’m a furry now,” and that’s what started it all. And I’ve been slowly learning more about the fandom ever since. I would like to participate more in the fandom when I’m older, but I’m too young for most of it right now. Guess I’ll just have to wait. But in the meantime, I can write about it in this text. I should sleep now. Goodnight. Hello, I'm back once again. Happy Pi Day! I memorized a bunch of digits of Pi once, not sure how many I still remember... I have literally nothing to write about now. I've been trying to come up with something for the past 10 minutes, and I still have no idea. Literally nothing is happening right now. It's pretty boring. My sister is watching Friends, as usual. Okay, since there's nothing for me to write about, I should go now. Bye! Wow, it has been a while since I last added to this. It is now July 10, 2019. Last time I edited this page was Pi Day, which was March 14. Those 4 months of this thing being untouched end today! Wait... 4 months? That means I was supposed to get this past the world record three months ago. Oh well. I have put many things into this text. A lot of them were cringy, like how I keep mentioning furry-related things. You know, I should stop putting things in here when I know I'm gonna cringe at them later. I'll try not to do that from here on out. I just know I'll fail though. I'd hate to be aware of someone reading this entire thing... like, if I had to sit and watch a family member or something read this entire text, I would cringe so hard. I would not want that to happen. I am currently pasting the entirety of the FlamingChicken LTE onto a page on OurWorldOfText. The frustrating thing about pasting stuff there is that it pastes one letter at a time, so it takes forever to paste long text. And when the tab isn't open, I'm pretty sure it just stops pasting, so you have to keep the tab open if you want it to continue. Why am I even doing this? No idea. I might not even paste the whole thing. I probably won't. Hey, I just had a thought. What if, in the future, students are reading this for a class assignment? What if this LTE becomes part of the school curriculum? If so, hi future student! I hope you're enjoying reading my CRINGE. What is my life coming to? That's enough writing for now. Goodbye. Hey again. Might as well continue writing in here for a bit. Hey, have you ever heard of 3D Movie Maker? It's a program from the 90s (that still works on modern computers) where you can make 3D animated movies. It's pretty cool. I've made a few movies with it myself, and many other people use it to make interesting stuff. In case you want to try it for yourself, I'm sure if you google "3dmm download" or something like that, it will take you somewhere where you can download the program. It's kinda aimed at younger children, but hopefully that doesn't stop you from making absolute masterpieces with this program. I have a keyboard in my room (the musical kind, not the one you type words on), and I don't really know how to play it properly, but I do it anyways. I can play a few songs on the piano (albeit with weird fingering because like I just said, I have no idea what I'm doing), including HOME - Resonance and PilotRedSun - Bodybuilder. You might not know one or both of those songs. If you don't know one of them, why not google it? You will have discovered some new music, and it will all be because of me. Why are you reading this, anyways? How did you even find it? Were you like me, and you were browsing pointlesssites.com, eventually finding the FlamingChicken LTE and going down a rabbit hole of discovering random LTEs? Literally the only reason I'm writing this right now is because that happened. I just discovered a new LTE: the RainbowFluffySheep LTE. I'm gonna see how many characters long it is. 75,957 characters. Pretty long, but not as long as the top two LTEs (FlamingChicken and Hermnerps, both with around 200,000 characters). I wanna write as much as possible into this text today. I'm gonna see how much LTE-writing I can do in one day. Hopefully it's a lot, because I wanna hold a world record! Imagine having a world record. Well, would it really be a world record? Because I don't know of any world record books that have "Longest Text Ever" as a record. Oh well, I just hope this LTE passes exactly 230,634 characters. That's all my goal is. I'm not even a tenth of the way there yet, but give it a month and I'm sure I'll get there. Hey, remember last time I said it would only take a month? That was four months ago. I should just stop promising things all together at this point. Forget I said anything about that. Did you know my sister has an LTE? That's right! It's not very long, though, and you can't read it because it's on her phone. She made it while bored at the library. That library was where I used to have web design classes. Those were fun, but I don't do them anymore. Now all I do it sit at home and write stuff in here. Well, I'm exaggerating. I go to the convenience store with my sister sometimes. But that's pretty much it outside of being bored on a computer. I should be a less boring human being. One day, I should translate this entire LTE into Viesa. That would be a big waste of time, even bigger than writing the LTE itself. But I could still do it. I don't think I ever will. This text is simply too long, and it'll be even longer than that by the time I pass 230,634 characters. By the way, if you think I'm gonna stop writing this once I pass 230,634 characters, you're wrong! Because I'll keep writing this even after I pass that point. It'll feel nice to be way ahead the record. My sister's alarm clock has been going off for half an hour and I haven't turned it off. Why? Because LAZYNESS! Actually, I really should turn it off now. There, I turned it off. First when I tried to turn it off, it started playing the radio. Then I tried again, and it turned off completely. Then I hurt myself on the door while walking out. So that was quite the adventure. I'm gonna go sleep now. Goodnight! Hey, I'm back again. My computer BSOD'd while writing this, so I have to start this section over again. That's why you save your work, kids! Before I had to start over again, I was talking about languages. Yes, I decided to bring that topic back after a while. But I no longer want to talk about it. Why? Because it'll probably bore you to death. That is assuming you're reading this at all. Who knows, maybe absolutely zero people will read this within the span of the universe's existence. But I doubt that. There's gotta be someone who'll find this text and dedicate their time to reading it, even if it takes thousands of years for that to happen. What will happen to this LTE in a thousand years? Will the entire internet dissapear within that time? In that case, will this text dissapear with it? Or will it, along with the rest of what used to be the internet, be preserved somewhere? I'm thinking out loud right now. Well, not really "out loud" because I'm typing this, and you can't technically be loud through text. THE CLOSEST THING IS TYPING IN ALL CAPS. Imagine if I typed this entire text like that. That would be painful. I decided to actually save my work this time, in case of another crash. I already had my two past attempts at an LTE vanish from existance. I mean, most of this LTE is already stored on Neocities, so I probably won't need to worry about anything. I think I might change the LTE page a little. I want the actual text area to be larger. I'm gonna make it a very basic HTML page with just a header and text. Maybe with some CSS coloring. I don't know. Screw it, I'm gonna do it. There, now the text area is larger. It really does show how small this LTE is so far compared to FlamingChicken or Hermnerps. But at least I made the background a nice Alice Blue. That's the name of the CSS color I used. It's pretty light. We're getting pretty close to the 1/10 mark! That's the point where we're one tenth of the way to making this the longest text ever, meaning all I have to do is write the equivalent of everything I've already written so far nine more times! Not gonna make any promises, though. How come every time I try to type "though", it comes out as "thought"? Why do I always type the extra T? It's so annoying that I have to delete the T every time. Okay, only mildly annoying. Not as annoying as I previously described. I apologize for my exaggeration of the annoyance level of me typing "thought" instead of "though". I just realized that most of the games I play are games that I've been playing for at least six years. I started playing Garry's Mod in 2013, Minecraft in whatever year version 1.2.3 came out. Now I have to look that up. March 2, 2012. So I started playing Minecraft approximately during that time. Wow, seven years ago! Coincidentally, I was also seven years old then. I remember the days of 2012-13. That was when I still played Roblox and made terrible YouTube videos. I was called "Infinite Budgets" back then. I also remember the days of 2016. A lot of people thought that was a terrible year, but for me personally, it brings me a lot of nostalgia because I talked a lot with my online friend at the time, and I did livestreams on YouTube and stuff. It was fun. 2016 was also when I got the phone that I still have to this day. Yup, my phone is three years old. My life was completely different when I got this phone: I was 11 years old, my YouTube channel actually had activity, and I wasn’t writing this text. I’m currently writing this in the car. We are on out way to the dollar store. And since I’m writing this on my phone, I’m making a lot more typos than usual. Some of them might make it through, so be prepared for that. Anyways, we appear to be getting close to the dollar store. I have a gift card for that place. I think so anyways, it might be for a different store... Yup, this dollar store is different. Oh well. My sister has an obsession with sponges. I’m sure she’s gonna find the sponges and go crazy over them. Why does she like sponges so much? No idea. She just found a bag of tiny baby dolls, and she wants to put them in ice cubes and call it “Ice Ice Baby”. She is truly a strange human being. My sister also has an obsession with stuffies. She has such an addiction, that she’s banned from them. Now she found the wigs and she’s considering buying one. She’s been looking at them for quite a while now. We’re out of the dollar store, and now we’re going to the computer store. I have no idea why we’re here. I guess we just are. Now we’re going home. Welp, that was a fun adventure. Stay tuned for more fun adventures as you read through this LTE. I should go now. Bye! Hello again. I made a private world on OurWorldOfText for my sister and I, but she doesn't want to join it. She doesn't think it'll be fun. Now I'm just editing it alone. How sad. But oh well. Now I’m here adding more to this text. I once made a Discord server specifically for a language called “Bo”, where the only word is “bo”. I made it almost four months ago, and somehow, it’s still going. People are still spamming nothing but “bo” there. It’s great. I also once made a server where you’re not allowed to use any vowels. It was a very strange server. I deleted it after some time though, so all that insanity is no more. I also used to own a Pig Latin server, but it got inactive so I deleted that too. We had some good memories in that server though. Now there’s a new Pig Latin server, but it’s not owned by me. Dang, my YouTube channel has been dead for so long. I haven’t posted a video in a year. I want to revive it, but I don’t know what to post there. I’ll figure it out. I doubt my channel will ever go back to it’s 2016 legacy, but I’m sure I’ll post something eventually. Random fact of the day: there are thirty-nine question marks so far in this text. Am I about to make it forty? Yes, I just did. Now the fact I initially stated is no longer true. Or is it? Because I said “so far” in the fact, that implies that we’re talking about the moment that fact was said, disregarding any future events. Now I’m pretty sure that fact is still technically true. Welp, I guess I should just accept that I’m editing that world of text alone for the rest of my life. I originally put a bunch of complaining in there, but I deleted it all. The thing is, now that world will never be same without all of that complaining about my sister not being here. But that’s fine. Hey, I just had a cool realization. Basically, there’s this conlang (constructed language, for those not in the know) server where we have a Sentence of the Week activity. In this activity, someone posts a text with a maximum of nine sentences, then people translate it into their own conlangs. My realization is this: if we take nine sentences from this LTE every week, there would be a whole year of sentences for people to translate. There are approximantly 523 sentences in this LTE. Divide that by 9 sentences each week, and you get 58 weeks worth of sentences, which is approximantly the number of weeks in a year. Quick maths. I actually suck at math, but that’s besides the point. I should go now. Goodbye! Hello, I’m back again. I really need to come up with different hello and goodbye messages, because I’ve already said “Hello, I’m back again” once before. Same with the “I should go now. Goodbye!” I said at the end of the previous section. I was going to explain what a “section” is, but I’m terrible at explaining things, so I’m not going to anymore. I guess you’ll just have to figure it out yourself. It’s probably not very hard to figure out, anyways. I guess I can just say that a section starts with me saying hello, and ends with me saying goodbye. That should be enough explaination, now that I think about it. Hey, do you ever feel like you never have any idea what you’re talking about? That’s my entire life. I just summarized it all in one sentence. On an unrelated note, I feel like half this LTE is just me talking about the LTE itself. I mean, press CTRL+F on this webpage, then type “LTE”. Look at all the times I use it in this text! Not counting the ‘lte’ in the word ‘multe’, of course. Dang, now the search results will include that, too. Anyways, half of this text is just me talking about how I’m trying to get this text to be the longest. Well, the longest LTE, anyways. I still have a long way to go. I’m only 12.7% of the way there. I mean, minus the four month gap, my estimation is that I’ve only been writing this for not even two weeks. So it makes sense that this LTE isn’t very long yet. Whenever I look at this webpage, it looks long at first glance, but the longer I look at it, the more I realize how short it actually is. It’s something that I can’t explain. For real this time. I just realized that none of this is helping the fact that half this LTE is about the LTE itself. I should bring up a new topic, but I don’t feel comfortable talking about much else. Why? Because, like I said, I never have any idea what I’m talking about. Most of this LTE is just me talking about LTEs or languages. Sometimes furries, but I don’t wanna go back into that territory at this point. But it doesn’t matter, because I’m still gonna write this LTE for as long as possible, even if it means talking about the same things half the time. Also, LEARN VIESA! Haven’t said that in a while, so I might as well bring it back. The documentation for Viesa is on this very website, so go ahead and read it! You might need to know some linguistic knowledge to understand it, though. In fact, you probably won’t understand most of it unless you know some amount about lingusitics, so you have been warned. If Viesa is too much for you, Pig Latin will probably be better for you. If it's so easy that kids can learn it, you can too! It's a language you can learn in probably five minutes, so why not give it a try? You may also enjoy Ubbi Dubbi, where you place 'ub' before every vowel sound. It's also a very easy language to learn, although not quite as popular. The thing is, none of these are even real languages. They're just codes, and very simple codes at that. You could probably crask Pig Latin or Ubbi Dubbi rather easily. Viesa too, actually. But I still enjoy them occasionally, even if Pig Latin and Ubbi Dubbi are inefficient and easy to crack, and Viesa is easy to crack yet unneccesarily difficult. I do make real languages, but I never put in the effort to learn them to fluency. At least I make them at all. Here’s a fun game: I will open up a random page from a book, and tell you the first word I see. English. That’s the word. Stay tuned for more fun games as you read through this LTE. We’re back, and we’re gonna play the same game as before. Ready? Subject. Now we’re gonna do it again. Reading. And again. Itself. Constituent. Grammar. Colloquial. Black. Outline. Add. About four of those words were language related. You’ll never guess why! (Spoiler alert: it’s a conlanging book). I’m running out of ideas now. I’m just gonna generate a random word and try to talk about it. Forbid. That’s the opposite of “allow”, I’m pretty sure. I don’t really know what else to say. Well, I guess I failed at generating a topic I could talk about. You know what's weird? My favorite word hasn't been used once in this entire text. I'm about to change that forever. Epic. Yup, my favorite word is "epic". I use it on a regular basis. I say "That's epic" all the time. It's a word I can't live without. Hey, I've now written more of this text after the 4 month gap than before it! Just thought I'd share that fact. Also, I'm gonna try and write as much as possible in this LTE today. I've already written more today than the day I first said I was gonna write as much as possible, so that's a good sign. The thing is, I don't know what to write about. I need to write about something, otherwise I won't write at all and I won't accomplish my goal. Wait, what goal should I set? How many characters should I write today? I'm gonna try and get 10,000 characters. I've already written almost 5,000 today, so from here I just have to write the equivellant of everything I've already written today. I'm just gonna try it and see if I make it. Maybe sometime in the future I'll do a bigger goal, like 15,000 or even 20,000 in one day. Actually, I don't know if 20,000 would even be possible for me. It might be, but it sounds like somewhat of a stretch for me to write that much in a single day. We'll see how long 10,000 takes, though. I'm already doing a bad job at this. I haven't typed anything here in several minutes. I need a topic. Um, Vabungula, I guess? Basically, it's a conlang created by Bill Price in 1965. It amazes me how one can work on a single conlang for that long. Most of the conlangs I start making die after 15 minutes. Anyways, I really like it because... um, I don't know, actually. There's not really anything about it that's super interesting (other than how long it's existed), it's just his personal conlang. Maybe it's the amount of development that went into it. It has over 5,000 dictionary entries and several texts written in the language. I'm sure most people reading this don't care about my language related talk, but I gotta make this long. I'm desperate to reach my 10,000 character goal. I've got 4,000 to go. I just found a website that generates random art from a seed. I just put this entire text as the seed, and it generated something quite nice. I would put the picture here, but I want this LTE to be nothing but text, so I won't do that. I've been playing with this for a while now. Many of the seeds produce boring pictures, but some of them are nice. For example, I just used "e" as the seed and it produced a nice looking picture. "a" looks nice too, arguably nicer. I've been using nothing but the word "nice" to describe these pictures. Maybe it's time to get a bigger vocabulary? "b" looks, um, good? I don't have the right vocabulary for this. I also don't feel like doing every single letter, because the pictures take some time to generate. But if you want to do it for yourself, just go to random-art.org and try it out! By the way, this is another website I found through pointlesssites.com. You know, the same website that lead me to the FlamingChicken LTE, which lead me to begin writing this whole thing. But what made me discover pointlesssites.com? Vsauce mentioned it. But what made me discover Vsauce? YouTube Reccomendations, probably. But what made me discover YouTube? As far as I remember, my dad showed it to me when I was 6. So I would like to thank my dad for being the reason I started writing this. He's the one who showed me YouTube, which reccomended me Vsauce, which mentioned pointlesssites.com, which brought me to the FlamingChicken LTE, which inspired me to start my own LTE. If he had never shown me YouTube, I wouldn't be here writing this text, and you wouldn't be reading it. Well, that's probably not true, because I probably would have discovered YouTube by other means, thus leading me to Vsauce, leading me to Vsauce, leading me to pointlesssites.com, leading me to the FlamingChicken LTE, leading me to... okay, I really need to stop now. I've gone too far. But you know what I haven't gone too far with? This LTE. I don't think I even can go too far with writing this text. Unless this text gets so long that it surpasses the 1GB storage limit of Neocities. In which case, I'll need to upgrade to Supporter in order to get a 50GB storage limit. But what if the text gets so long that is surpasses that? I don't think I'll ever make it there. I mean, 50GB is about 50 trillion characters. So I think we're good. I still need to get to 10,000 by the end of today. I've got 1,500 to go. Currently watching a livestream. It's reminding me of when I used to livestream back in 2016. I still kinda miss those days. But at the same time, I was quite awkward and had zero social skills, so I'm not sure if I'd want to go back. At this point, everything I've written today is longer than what can fit on the screen at once. At least on my computer screen. It probably changes with different screen resolutions and devices. But anyways, it's pretty unusual for that much of the LTE to be written in a single day. I don't want to pressure myself into writing this much every day, though. Last time I forced myself to complete a certain amount of something every day, it was overwhelming and I ended up losing motivation, thus letting down all my fans who were anticipating the August 30th, 2016 release date. Okay, the amount of eager fans was probably a number you could count on one hand, but still. By the way, if you're wondering what this "something" was, it was GoAnimated Garbage: The Movie, which was supposed to be an hour long episode of a series I made to make fun of random GoAnimate videos. In case you're not the type of person who knows what GoAnimate is... hoo boy. Basically, it's a drag-and-drop animation website infamous for the "grounded videos" that people made with it, among other types of videos. It's this whole community that I neither can explain nor want to explain. But I had somewhat of an association with that community back in the day. On my YouTube channel, I used to make a genre of GoAnimate video known as the "OS video". Typically an OS video is where some sort of hated character within the GoAnimate community forcefully installs their operating system onto a user's computer, and the user has to deal with this OS until they eventually find a way to "destroy" it. I made five of these videos. In chronological order: Caillou OS, Boots OS, Franklin OS, Little Bill OS, and Crap OS X. Caillou OS is the most viewed video on my main channel, which is unsurprising since Caillou is pretty much THE character associated with the GoAnimate community. When I made that video, it was a big transition for my channel. The channel's name was changed from Infinite Budgets, which had been my name since 2013 when I made crappy Roblox videos, to Allisima. All of my old videos were deleted, with the exception of my "Barney Errors", which was yet another genre of GoAnimate video. Basically, a Barney error is when a user's computer/console/whatever session is interrupted by a "Barney Error", a message informing the user that Barney has been killed, and the device must not be turned off because it's an "important message". There's also a bomb that's placed in Barney's "lair", the timer for which is displayed in the error. The user gets some amount of "chances", and every time the device is turned off, the user looses a chance and the time until the bomb explodes decreases. Eventually, the user turns off the computer enough times that there are no more chances left, the bomb explodes, and some sort of punishment happens. These punishments can range from having to downgrade your operating system, to having your computer destroyed, and in extreme cases, even to death. I once made a whole channel for Barney Errors, where I made about twenty of them before quitting. After that, I eventually quit GoAnimate all together, but I still made Crap OS X, an OS video made with Powerpoint. I also made an interactive OS parody called Windows Poop Editon, again with Powerpoint. Before that, I also made one called "Atch OS" using my old Windows XP netbook. I just checked to see if my old Weebly website still exists, since there's an Atch OS download on there and I wanted to see if it dissapeared from existence or not. Appearantly it does! I'm getting so much nostalgia from this website. It's like a window into 2016, when I had fun making these videos on a regular basis. I'm way past my 10,000 character goal now. I'm kinda glad I set this goal, but again, I'm not gonna force myself to do it everyday. I think I'm gonna stop writing for today. Bye! Hey, I'm back. Yes, that hello wasn't original either, since I already said it once. Specifically, after my sister seized the LTE and started spamming. You remember that, right? I hope you read through this whole thing instead of just picking a random part (which just happened to be this part) and reading only a tiny bit. Nah, I'm just kidding. Read this text however you want to, it doesn't matter if you read this entire text from start to finish or not. I mean, I did put some cringy stuff in here, as I keep mentioning. But it's on the Internet, and since recently, on my homepage, so I know people are gonna read it. Really the only reason I'm making this is because I have a weird obsession for writing giant walls of text. Guess what? I just added translations of this LTE into various conlangs on my website! But they're all very incomplete, and I probably won't finish them ever... I mean, if I'm gonna finish any of them, 'twill probably be the Viesa translation since it's the easiest to do. Hey, 'twill's back! I remember the very beginnings of this LTE, when I first mentioned 'twill. That was 40,000 characters ago. Appearantly I'm measuring time with characters now. Hey, what's the average amount of text I write per day in this LTE? The four month gap probably significantly drops that amount. Let's see! The trouble is finding out when I started writing this LTE, because I don't know the exact date. I'm just gonna estimate that it was March 12, based on the amount of times I said goodnight before I said "Happy Pi Day". It's not a very accurate measurement, though, because sometimes I stop writing for the day without saying goodnight. But anyways, from March 12 to today, July 16, is 127 days. As of that previous sentence, there are 42,549 characters in this LTE. 42,549 characters divided by 127 days equals about 335 characters per day. That's not very much at all. To get an idea of how short that is, the first 335 characters of this LTE consist of about 64 words and 8 sentences. As I predicted, the four months of no activity had a big impact on this number. But what if we ignore the 4 month gap, which was from March 15 to July 9, I've only been working on this LTE for ten days. 42,549 characters divided by 10 days is about 4254 characters. That's much better. It might be that big because of the 12,600 characters I wrote yesterday. I said I wouldn't do it every day, but honestly, I'm feeling like doing a goal again today. I think I might even go a bit higher than yesterday. Let's do 15,000 characters! I have zero life outside of this LTE, anyways, so I think I'll make it. As long as I keep typing about random stuff for the entire day, I'll probably get past 15,000 easily. I think I'm insane. Literally all I do anymore is write this LTE. My mom is almost certainly concered for me, because I was in my room pretty much all of yesterday and my sister told her about how I'm trying to write the longest text ever. But enough about my descent into insanity for now. Let's get this LTE to over 55,000 characters today! This is probably the most meta LTE in existence. Like I've said, I talk about the LTE itself as much, if not more than anything else. By the way, if I were to write as much as I did yesterday every day, I would reach my goal in just 15 days. Now I'm tempted to do that, even though I said I wouldn't set a goal like that every day. I think I might end up doing it subconciously. I kinda wanna convince some other people I know online to start their own LTE. Wouldn't it be fun if we all had our own LTEs? They would probably all die within a day, but at least I wouldn't be the only one writing an LTE in 2019... The most recently updated LTE I've seen is the RainbowFluffySheep LTE, which I believe was last updated in late 2018. That wasn't really that long ago, but still, I don't think it's being updated anymore. Now let's do an LTE Timeline! The original FlamingChickens LTE was probably started sometime in 2004, and Hermnerps was started the same year. The FlamingChickens LTE stopped in 2005, while the Hermnerps LTE actually lived on until 2009, although edits after the end of 2004 were rather sparce. The Kenneth Iman LTE was started in 2013 and was last updated in 2015. The RainbowFluffySheep LTE both started and was last updated in March 2018. And of course, the WhileTrue LTE was started in March 2019 and is still being updated today. Wow, 15 years of LTEs! I think my LTE is the only one still being updated. It would be nice if someone else was writing their own LTE along with me. But 'twill be hard to convince other people to waste their lives writing a useless wall of text. You never know, maybe an LTE that stopped being edited years ago will come back from the dead. That seems kind of unlikely though. Very strange fact incoming. A certain word has not been used since the very beginning of this text. Ready to learn what it is? I shouldn't tell you, actually. Of course, that would ruin it. Unless you want me to ruin a really cool fact. Surely you wouldn't want that to happen. Okay, I'll just tell you, because I'm probably gonna end up using it again someday or another. The word is "various". If you search for "various" in this LTE, you'll only find it at the very beginning as well as here. And I was gonna keep this a secret, but just now I did this thing where if you take the first letter of each sentence, it spells out "VARIOUS". Kinda clever... I guess? Anyways, for those who are insane enough to be reading this entire thing from the start: Wow, you have quite the dedication. My LTE isn't even the longest yet, but perhaps in the future, when it is the longest, people will be challenging themselves to read the entire thing. And maybe you're one of them! Perhaps you're reading this long after I've passed my goal, in which case you still have quite a bit to go. So I wish you luck on your Longest Text Ever reading adventure! I've been talking about LTEs all day. For the past 6,000 characters, in fact. I need to find something different to talk about. But first, I just had an idea pertaining LTEs. I should compare this LTE to the longest joke in the world! The longest joke in the world is 56,554 characters long, which is about how long I'm trying to get this LTE by the end of today. So if I reach my goal today, this text will be longer than the longest joke in the world! That's pretty cool. I would also be a quarter of the way to my goal. But let's get back to finding something different to talk about. I can't think of anything. My sister is singing a song about wanting Subway. I will never understand her. What goes through her brain that makes her decide "Yeah, I think it would be a good idea to sing about how I really want Subway"? I don't get how her brain works. She also likes eating paper. I asked her and appearantly she was perfectly okay with me writing that in here. She probably thinks nobody's ever gonna read this. But she's gonna be wrong! Eventually. Now she's asking me to write about how she likes yogurt. "Because I didn't used to", she says. She's eating mango yogurt, and she has water in a Gatorade bottle. Now I'm asking her what else I should put in this text. She says I should write about how there's wild sage where we live. Now she's having hot chocolate. She didn't ask me to write that, but I told her I was going to write it and she said okay. My sister might start her own Longest Text Ever, again. She says it will have only one word repeated throughout the entire text. But I told her that it defeats the purpose of an LTE. In the original FlamingChickens LTE, one of the very first things that is written is "I will just type, and type, and never, ever use copy and paste". Okay, I just made a webpage for her LTE (it's gonna be an actual LTE this time). Stay tuned for "The Best Longest Text Ever", as she calls it. I think it should have just been called "KKs Longest Text Ever" or something, but whatever. She types really slow, but I hope her LTE will be successful nonetheless. Warning: if you do go and read her LTE, she spoils Spiderman: Far From Home at the very beginning, so be careful about that. In fact, she's basically typing the entire plot of the movie. Well, that's one way to increase your LTE's length, I guess. My sister is listening to her terrible songs instead of writing her LTE. Well, she has her LTE page open, but she's not writing anything and is singing instead. Actually, she's writing stuff now, so ignore everything I said previously. She's still writing the entire plot. Her LTE is now 2,000 characters, which isn't very long, but she's only been working on it for an hour. Plus she's a slow typer. She types everything with one hand. It might take a while for her LTE to get to this level. But assuming she keeps writing it and doesn't forget about it after today, it'll get pretty long eventually. I still need to write 7,000 characters today. My sister is watching a cringy video made by our old elementary school. They became a French immersion school after I left. She found one of the videos I was in... oh god, I can't stand to look at that video. It hurts me to think about those days. My sister's LTE webpage has text now! Maybe I should create a page linking to all the LTEs I know about. I think I'll do that. Boom, it is done. I think I'm gonna also put a link to it on this page. There, that's done as well. Guys, I'm not sure if I'm gonna make it to 15,000. I still have 5,000 characters to go (I was completely off earlier, I don't have 7,000 left to go), and there's not much left of the day. In retrospect, it was probably a bad idea to make a goal for the day in the first place. After all, LTE writing is supposed to be fun! Sort of. There's zero need to make unneccesary deadlines. I think it just reduces the fun, as well as the part of my life that isn't just writing huge walls of text. From here on out, I declare character-per-day goals abolished. I will no longer make attempts to write a certain amount in a single day. I should have listened to my past self, who said not to do goals every day. But I didn't, and now I regret it. But anyways, here's a fun fact about this LTE: excluding my upcoming usage, the pronoun "he" is only used twice in this LTE, and they both refer to my dad. On the other hand, the pronoun "she" is used forty times! Almost all of these refer to my sister. Only one refers to my mom. I guess I just really like talking about the weird stuff my sister does. But not as much as being meta and talking about my own LTE. Here's another fun fact: "LTE" is the fourteenth most common word in this text! That's insane. It's more common than words you'd expect to be common, like "you", "I'm", "for", "be", "about", "was", and so on. I really need to talk about other things once in a while. But since I have zero creativity, I always resort to talking about the same topics. From what I've seen, most other LTEs are pretty diverse, but mine isn't at all. Honestly, this is likely the most boring LTE to read. But my absolute lack of creativity means it's probably gonna stay that way for a long time. I'm tired, so I'm gonna go to sleep. Maybe I'll be more creative by tomorrow. Probably not. Anyways, goodnight. Hey, I'm back, and I don't feel any more creative. But I did have a dream last night, so I'm gonna talk about that. Last night, I dreamt that I was in one of our old houses, and I saw that someone made a video roasting Viesa. They talked about how you shouldn't say "dog" in Viesa, because appearantly "deeg" is bad or something? I don't know. Then they said the rule where W becomes V is weird, but I don't remember the reason they said it. I didn't really care about how they roasted my language. Then I watched a Minecraft video for whatever reason, and then the dream ended. How do other LTE writers have so many topics to talk about? All I ever talk about is either LTEs themselves, or the fact that all I ever talk about is LTEs. There's no diversity. I very rarely talk about anything else. And when I do, it's usually about languages and lasts only a few sentences. There, I deleted it. Oh, you don't have any context. Basically I wrote a bunch of depressing stuff, then I decided to delete it all. I knew I was going to regret it later, in the same way I regret writing all that stuff about furries. Not that I think there's anything wrong with being a furry, it's just that it personally makes me uncomfortable looking back on it. I'm not even into that stuff as much anymore. I don't watch furry YouTube, and I don't talk about how much I want a fursuit/go to a convention. That's a part of me that's slowly disappearing. Okay, I'm gonna stop talking about that, because I literally just said how I regret talking about it in this text. You know, I've been feeling kind of down about this LTE lately, because as I just mentioned, all I ever talk about is this LTE itself, there's no diversity, blah blah blah. It's especially been like that ever since the four month gap. In fact, I barely talked about LTEs before that gap. It's like I lost all my creativity after four months. You know what? I'm officially gonna say this: If, for some reason, you are reading this before you decide you want to start reading this entire text, READ EVERYTHING FROM "WOW, IT HAS BEEN A WHILE" TO HERE AT YOUR OWN RISK, BECAUSE YOU WILL LIKELY DIE OF BOREDOM DUE TO THE MONOTONOUS TOPICS! There, now I'm gonna try and forget that half this LTE is the same exact boring topic. I will also try to avoid writing about the same exact boring topic for the rest of this text. Let's celebrate the End of Monotonous Topics (EMT) by talking about how we (my sister and I) had lunch and did various other things with our grandpa! So grandpa asked if we wanted to have lunch and spend an afternoon with him, and we said yes. Then he picked us up, and we went to a nearby town where we had lunch, went to a museum which was a house built in 1909 as well as the town's first hospital, and got ice cream from what is appearently one of the best ice cream places in the country, according to grandpa. So today was a fun day. I'm gonna go now. Bye! Hey, I'm back. That's the fifth time I've said that. I need to come up with more original... nah, whatever. Anyways, I had a dream last night which was basically a whole movie I don't remember most of. All I remember is playing a keyboard at the store for some reason, and that the dream ended with a random car horn. Oh, and there was Minecraft involved in the beginning, which I'm pretty sure is becoming a recurring theme in my dreams. I don't know why that happened, because I rarely play Minecraft anymore. Do any of y'all remember the DVD screensaver meme? That was one of my favorite memes. For those who don't know what I'm talking about, many DVD players had this screensaver where it was a DVD logo bouncing around the screen. The big moment that everyone anticipates is when the logo hits the corner of the screen perfectly, because, well, it's just so SATISFYING! I used to watch a livestream that was literally just this screensaver running endlessly. And when it hit the corner, it was a huge celebration for both me and everyone else watching. I got so excited when the logo hit the corner. My computer's screensaver is even still a DVD screensaver. But nowadays when I see it hit the corner, I don't have as much enthusiasm as I used to. I've just seen it too many times for it to be exciting anymore. Plus, the meme isn't even a thing anymore. I doubt that livestream is even still running. But you never know, so I'm gonna check to see if it's still going. Oh wow, it is! That was the last thing I expected to see in July 2019. But only four people are watching it, which makes sense. The title now says "DVD Logo Screensaver For 1 Year", even though it hasn't quite been going on for a year. But when it hits that point, perhaps that's when it will finally end? It should have ended months ago, if you ask me. Yup, I was right. There's a countdown on the livestream to when it ends, and it says 181 days, 9 hours, 12 minutes, and 3 seconds. Wow, the corner hit and wall hit numbers are much bigger now. The most corner hits I'd seen is around 1400 or so, but now it's at 4776! The wall hits used to be in the hundred-thousands, now it's at over two and a half million! Hello, I have returned. There, I came up with something original to say! Anyways, I just combined every single LTE I know of (including this one) and put it onto one single page on a Wikia wiki called "No Rules Wiki". That wiki exactly as you would expect from the title. I found it a while ago, and I thought it was about time I made a contribution, even if pasting over half a million characters into a single article is breaking some rule... I've been wanting to make Viesa an actual conlang for so long now. I think it's long overdue at this point. Hey, I'm back again. These sections are getting shorter and shorter each day. But oh well. I just discovered how much I like the word "number". I don't know why, but it's just so fun to say! I think I've liked that word ever since I was a toddler learning my numbers! I remember thinking it was a fun word even back then. At that time I had two little electronic toys: one was orange and for numbers, and one was purple and for letters. I'm pretty sure those were the colors. I also vaguely remember having a fan that lit up and displayed custom messages. I haven't seen anything like that since then. All I hear right now is Baby Shark being blasted upstairs. You know that song, right? I don't know who doesn't know it at this point. I can't think of a single person I've seen that doesn't know what that song is. Dang, ever since the EMT I haven't been writing as much in this text. Looks like LTEs were all I could talk about. Oh well. How many times have I said "oh well"? Probably a lot. About eight times, in fact. I'm back again. I went a full day without writing anything into this LTE yesterday! There were a lot of things happening that day, so I didn't feel like writing. I could've written at least a little bit, but I didn't. Time for me to use this LTE as my dream journal yet again! I had a dream where my domain was "exin" (or something like that) instead of "whiletrue", so that was a thing. I also had a dream where there was this game that I thought existed in the real world, but it didn't. Dreams do that sometimes. I don't remember much about the game, but it involved the Simpsons, I guess? Also, I was in a weird store where they had an... iCarly laptop? And a bunch of gift cards. That's all I remember. For now, at least. My sister does not like synthwave. She says "it's repetitive", "the sounds they use don't sound like music", and she doesn't like how it doesn't have lyrics. First of all, she's hypocritical because she always listens to the same songs on repeat. And why does it matter that it doesn't have words? Why does she think every single piece of music in existence has to have words? YOU BETTER WATCH YOUR OPINIONS THERE! (That was a reference to a cringy GoAnimator that no one reading this will get, unless you came to this website from my YouTube channel which you subscribed to during my OS video days). Anyways, synthwave is objectively the best genre of music. I remember hearing HOME - Resonance for the first time in a Discord voice chat, and it was magical. I wish I could listen to that song for the first time again. That was how I got into synthwave. You know what my favorite color combination is? Yellow text on a magenta background. Oh, and don't forget the Comic Sans. That is just pure beauty right there. In fact, it's used in the first frame (well, close enough) of "history of the entire world, i guess", which makes me love that video even more. We're at 60,000 characters, 1,000 sentences, and 12,000 words! Weird how all those counts hit such round numbers in one day, huh? I need to stick to the EMT, so I should stop talking about that. My sister is attempting to build a Lego city. Her goal is to have three buildings, since she doesn't have THAT much Lego. Have you noticed how quickly I've been switching topics in this text? That's because I can't talk about anything for a long time. That is, unless that thing is languages or LTEs. I am currently trying to revive a language my sister and I started making a while back. Sometimes my sister has days when she doesn't hate languages for some reason, then she ends up starting one. But of course, she regained her hate and abandoned it. Now I'm the only one working on the language. By the way, the language is called Lazay, which was the successor to Zula, the first language we made together which is now deleted. We started writing the language on paper, but then I started a Google Doc. I'm sure the papers are still here somewhere. I'm just too lazy to find them. I’m back again. I haven’t been ending these sections with goodbyes recently. But whatever. We’re on our way to IKEA to get a dresser for my room. We’re listening to Queens of the Stone Age right now, and I’m just waiting for “Fortress” to come on. I sing that song in Viesa, but I make up half of the lyrics. It goes: Ванавар јак фиртрас кува, ма башег ђара, ја сок. Try and translate that! The song is playing now. I like this song. We’re back from IKEA now. Actually, we’ve been home for hours now, and we’ve already built the dresser. My computer crashed (but don’t worry, I started writing this in Google Docs on my phone), and now Google Chrome won’t open. So I have to use Microsoft Edge for now. I’m gonna sleep now. Goodnight! Hello, I'm back. My sister is brushing my back with a hairbrush, and I don't know why. I asked her what I should write about (because I have zero creativity), and she said I should write about that. I'm gonna type whatever comes to my head now. Hi, I'm a boring human being who has zero creativity whatsoever and still happens to be writing an LTE. Isn't that insane? How could this be? Nobody knows, and nobody will ever know. It is a strange mystery that has yet to be solved. Hmm, I wonder if I should go and eat pancakes now? I'm so random right now. In fact, there's an entire subreddit for that: r/iamsorandom. You should check it out! I mean, you don't really have to, but it would be nice if you did. I use Reddit a lot, but I only use it for language-related stuff. Well, I make posts in language-related subreddits, but the non-language subs that I look at are ones that I don't post anything to, because I know nothing about literally anything that isn't languages. And heck, I don't even know much about languages! I only make English codes and call them "conlangs". Sort of. I usually don't actually call them conlangs, but I use them for such purposes. I speak Viesa as if it were a real language, but it simply is not. Why did I make Viesa in the first place? Well, you see, it all started out as a joke for April Fools' Day. I called it "the new universal language", despite it literally being a cipher of English. What!? A cipher of English being a universal language? How silly! What a funny joke, right? Maybe? Somewhat? Anyways, I then made a SECOND VERSION! DUN DUN DUN! This second version had CLICKY SOUNDS which, spoiler alert, dissapear in the next version of Viesa. Sad, right? RIP CLICKS 2018-2018 NEVER FORGET! I also added WACKY GRAMMAR STUFF and PRONOUNS! WOAH! How crazy! Then I made the next version: VERSION 3.0! This version added CYRILLIC! (you know, that alphabet the Russians use, as well as the Serbs, whose version of the Cyrillic alphabet I stole for Viesa. Hehehe!) And that's the entire history of Viesa, explained in a Zany way! Do you like how I capitalized "Zany" there? Aren't capital letters so cool? They let you YELL AND SCREAM AT THE TOP OF YOUR LUNGS! They add EXCITEMENT! And most of all, they let you capitalize words like This. lowercase letters are also cool. without them, we'd all be yelling and screaming all the time. That would be pretty tiring, wouldn't it? I see two water bottles. One is empty, while the other still has some water in it. The empty one is blue, and the one with the water is pink. I should also mention that the blue one is mine, while the pink one is my sister's. I got that water bottle because I lost my other one at school. But GUESS WHAT? I FOUND IT IN THE LOST AND FOUND! Wow! Now I had two water bottles. How Wacky and Crazy and Zany and Bizzare and all those adjectives that perfectly describe this epic moment! Wow, writing your mind is a great way to increase your LTEs length! Before I was actually THINKING about what I was writing. But now I barely do, and it's greatly improving my LTE! Except the overuse of capital letters might throw the reader off guard a little because of how sparingly I've used them in the past, but oh well. I could fix it, but I don't feel like it. I want to continue writing, but I need to sleep now. Goodnight! Hi, I'm back again. My computer crashed AGAIN, and I was ignorant enough to not save my work, so that means I have to start this part of the text all over again. That's quite unfortunate. But did I mention that my Google Chrome is working again? That's the good news. It's good news because Google Chrome has all my logins, websites, and stuff like that. Hopefully you know what I mean when I say that. Maybe you do, maybe you don't. I don't even know what I mean right now! I'm probably insane right now. Especially since I'm writing this right now, as I have been for about 18 days minus the four month gap... I think. I hope I did that right. As I've said before, I'm bad at math. My sister just read the entirety of what I've written today for some reason. My sister just sang "I want your computer to crash again because I'm evil". She IS evil if she wants my computer to crash. At least I'll have this section saved. In fact, right now I'm pressing Ctrl+S after every sentence! Including this one. And this one. Also this one. I think you get the point now. My sister keeps typing into this LTE without my consent, and I keep having to delete it all. It's pretty annoying. Hey, flashback to when I said that way at the beginning of this text! You know, the part where I talk about the Teen Titans Go episode called "Waffles" where the word "Waffles" is said a hundred-something times. You know what else is said a hundred something times (in this LTE)? The letter J. So far it's been used 115 times in this LTE. That's your Interesting LTE Fact of the Day! Well, not really "daily", but whatever. Here's a story: Once upon a time, people got tired of starting off their stories with "Once upon a time", so they stopped doing that. But one person decided not to stop using "Once upon a time", and used it at the beginning of this story. And that person is ME! The end. Wasn't that a lovely story? You're probably not thinking that. Again, I'm not creative in any way whatsoever. That's why I don't usually write stories and instead write giant walls of text full of meaningless information, like the one and only WhileTrue's Longest Text Ever that you're reading right now. Hopefully nobody died of boredom from reading between "Wow, it has been a while" and the EMT. That's the most boring part of the LTE! 90% of it is just me talking about LTEs themselves. How uninteresting is that? Very uninteresting. Penguins. What are they? I don't know. What am I even writing right now? I haven't a clue. Isn't it weird that I said "haven't a clue" like that? Normally "haven't" isn't used if it's alone as a verb, as in "I haven't my keys". Who says that? Nobody, that's who. And yet "I haven't a clue" is an actual thing I've heard people say. Anyways, AFRICA! That was random, but let's discuss it anyway. Africa is a well-known song by Toto. It's a good song. I can kinda sorta play it on piano? Maybe? I don't know. Another song I can play on the piano is All Star by Smash Mouth. You know, the Shrek song? Anyways, I once made a video called "All Star but it's played on a Sesame Street piano" and it got almost a million views. It's been stuck at 900,000 for what seems like forever now. I'm gonna check to see if it's at a million now. I doubt it, though. Nope, still at 926,000 views. And I doubt it's gonna get any more, to be honest. It had a good run though. My sister is chugging applesauce. She thinks she's epic because of it. I don't know anymore. I seem to keep saying that after everything I type at this point. It's strange. Hello, I have returned after yet another long absence. When was the last time I added to this? I think it was somewhere in July. So yeah, it’s been three months, as it is now October 17, 2019. The end of the decade is approaching fast. I’m a bit excited, because I’ll have significant memories from more than just one decade! My earliest significant memories started in Kindergarden, which was in 2010. This means that I only really remember one decade. But now that an entirely new decade is coming up, I’ll be able to remember another! Part of me feels like I shouldn’t be excited over this, since the boundaries between years is arbitrary, and a decade is 10 years only because we count in base 10, so if we counted in base 12 or something, a decade would be 12 years long. That was kind of a run-on sentence, but I don’t really feel like making this text perfect, anyway. Have you heard of the Library of Babel? libraryofbabel.info is a website containing every possible combination of the lowercase letters a-z, space, comma, and period. The library is divided into hexagonal chambers. Each hex contains four walls. Each wall contains three shelves. Each shelf contains 32 volumes. Each volume contains 410 pages of 3200 characters each. Everything you could ever say or write is on this website. Even this LTE! See for yourself: https://libraryofbabel.info/bookmark.cgi?lte. Okay, that’s only the first bit of it, but every other bit of this LTE is somewhere in the library! In fact, here’s the next bit: https://libraryofbabel.info/bookmark.cgi?lte:1. It’s split up into about 20 different pages. I don’t feel like putting links to all of them here. It also removes punctuation that the library doesn’t use, like the exclamation point, question mark, colon, and so on. But it’s pretty mind-blowing stuff, if you ask me. If you try and browse the library yourself though, you probably won’t find much more than total gibberish. It’s crazy to think that everything we could ever possibly say or write is massively outweighed by meaningless strings of letters and punctuation."""


seq_length = 100# sequence length
sequences_step = 5# sequences_step to create sequences

chars = sorted(list(set(text)))
char2idx = {w: i for i,w in enumerate(chars)}
idx2char = {i: w for i,w in enumerate(chars)}

def vectorization():
    # cut the text in semi-redundant sequences of seq_length characters
    sentences  = []
    next_chars = []
    for i in range(0, len(text) - seq_length, sequences_step):
        sentences.append( text[i : i+seq_length])
        next_chars.append(text[    i+seq_length])

    n_sentences = len(sentences)
    n_chars = len(chars)
    # print("Sentences size: ", n_sentences)

    x = np.zeros((n_sentences, seq_length, n_chars), dtype=np.bool)
    y = np.zeros((n_sentences, n_chars), dtype=np.bool)
    for i, sentence in enumerate(sentences):
        for t, char in enumerate(sentence):
            x[i, t, char2idx[char]] = 1
        y[i, char2idx[next_chars[i]]] = 1
    
    return x, y
# x, y = vectorization()



# # import numpy as np
# import sys, os
# sys.path.insert(1, os.getcwd() + "./../../network") 
# from layers import LSTM


# # # generate data
# # data = open('./wonderland.txt', 'r').read()
# # chars = list(set(data))
# # data_size, vocab_size = len(data), len(chars)
# # print(f"data has {data_size} characters, {vocab_size} unique.")
# # char_to_ix = { ch:i for i,ch in enumerate(chars) }
# # ix_to_char = { i:ch for i,ch in enumerate(chars) }

# # # hyperparameters
# # hidden_size = 100 # size of hidden layer of neurons
# # seq_length = 25 # number of steps to unroll the RNN for
# # learning_rate = 1e-1

# # # model parameters
# # Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
# # Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
# # Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
# # bh = np.zeros((hidden_size, 1)) # hidden bias
# # by = np.zeros((vocab_size, 1)) # output bias

# # def lossFun(inputs, targets, hprev):
# #   """
# #   inputs,targets are both list of integers.
# #   hprev is Hx1 array of initial hidden state
# #   returns the loss, gradients on model parameters, and last hidden state
# #   """
# #   xs, hs, ys, ps = {}, {}, {}, {}
# #   hs[-1] = np.copy(hprev)
# #   loss = 0
# #   # forward pass
# #   for t in range(len(inputs)):
# #     xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
# #     xs[t][inputs[t]] = 1
# #     hs[t] = np.model.activation(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
# #     ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
# #     ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
# #     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
# #   # backward pass: compute gradients going backwards
# #   dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
# #   dbh, dby = np.zeros_like(bh), np.zeros_like(by)
# #   dhnext = np.zeros_like(hs[0])
# #   for t in reversed(range(len(inputs))):
# #     dy = np.copy(ps[t])
# #     dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
# #     dWhy += np.dot(dy, hs[t].T)
# #     dby += dy
# #     dh = np.dot(Why.T, dy) + dhnext # backprop into h
# #     dhraw = (1 - hs[t] * hs[t]) * dh # backprop through model.activation nonlinearity
# #     dbh += dhraw
# #     dWxh += np.dot(dhraw, xs[t].T)
# #     dWhh += np.dot(dhraw, hs[t-1].T)
# #     dhnext = np.dot(Whh.T, dhraw)
# #   for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
# #     np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients
# #   return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

# # def sample(h, seed_ix, n):
# #   """ 
# #   sample a sequence of integers from the model 
# #   h is memory state, seed_ix is seed letter for first time step
# #   """
# #   x = np.zeros((vocab_size, 1))
# #   x[seed_ix] = 1
# #   ixes = []
# #   for t in range(n):
# #     h = np.model.activation(np.dot(Wxh, x) + np.dot(Whh, h) + bh)
# #     y = np.dot(Why, h) + by
# #     p = np.exp(y) / np.sum(np.exp(y))
# #     ix = np.random.choice(range(vocab_size), p=p.ravel())
# #     x = np.zeros((vocab_size, 1))
# #     x[ix] = 1
# #     ixes.append(ix)
# #   return ixes

# # n, p = 0, 0
# # mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
# # mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad
# # smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0


# # while True:
# #   # prepare inputs (we're sweeping from left to right in steps seq_length long)
# #   if p+seq_length+1 >= len(data) or n == 0: 
# #     hprev = np.zeros((hidden_size,1)) # reset RNN memory
# #     p = 0 # go from start of data
# #   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
# #   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

# #   # sample from the model now and then
# #   if n % 100 == 0:
# #     sample_ix = sample(hprev, inputs[0], 200)
# #     txt = ''.join(ix_to_char[ix] for ix in sample_ix)
# #     print(f"----\n {txt} \n----")

# #   # forward seq_length characters through the net and fetch gradient
# #   loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
# #   smooth_loss = smooth_loss * 0.999 + loss * 0.001
# #   if n % 100 == 0: print(f"iter {n}, loss: {smooth_loss}")
  
# #   # perform parameter update with Adagrad
# #   for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], 
# #                                 [dWxh, dWhh, dWhy, dbh, dby], 
# #                                 [mWxh, mWhh, mWhy, mbh, mby]):
# #     mem += dparam * dparam
# #     param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

# #   p += seq_length # move data pointer
# #   n += 1 # iteration counter 




# import random
# import numpy as np

# def model.sigmoid(x): 
#     return 1. / (1 + np.exp(-x))

# def model.sigmoid_derivative(values): 
#     return values*(1-values)

# def model.activation_derivative(values): 
#     return 1. - values ** 2

# # createst uniform random array w/ values in [a,b) and shape args
# def rand_arr(a, b, *args): 
#     np.random.seed(0)
#     return np.random.rand(*args) * (b - a) + a

# class LstmParam:
#     def __init__(self, mem_cell_ct, x_dim):
#         self.mem_cell_ct = mem_cell_ct
#         self.x_dim = x_dim
#         concat_len = x_dim + mem_cell_ct
        
#         # weight matrices
#         self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)
#         self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len) 
#         self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)
#         self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

#         # bias terms
#         self.bg = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bi = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bf = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bo = rand_arr(-0.1, 0.1, mem_cell_ct) 
        
#         # diffs (derivative of loss function w.r.t. all parameters)
#         self.wg_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wi_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wf_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wo_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.bg_diff = np.zeros(mem_cell_ct) 
#         self.bi_diff = np.zeros(mem_cell_ct) 
#         self.bf_diff = np.zeros(mem_cell_ct) 
#         self.bo_diff = np.zeros(mem_cell_ct) 

#     def apply_diff(self, lr = 1):
#         self.wg -= lr * self.wg_diff
#         self.wi -= lr * self.wi_diff
#         self.wf -= lr * self.wf_diff
#         self.wo -= lr * self.wo_diff
#         self.bg -= lr * self.bg_diff
#         self.bi -= lr * self.bi_diff
#         self.bf -= lr * self.bf_diff
#         self.bo -= lr * self.bo_diff
#         # reset diffs to zero
#         self.wg_diff = np.zeros_like(self.wg)
#         self.wi_diff = np.zeros_like(self.wi) 
#         self.wf_diff = np.zeros_like(self.wf) 
#         self.wo_diff = np.zeros_like(self.wo) 
#         self.bg_diff = np.zeros_like(self.bg)
#         self.bi_diff = np.zeros_like(self.bi) 
#         self.bf_diff = np.zeros_like(self.bf) 
#         self.bo_diff = np.zeros_like(self.bo) 

# class LstmState:
#     def __init__(self, mem_cell_ct, x_dim):
#         self.g = np.zeros(mem_cell_ct)
#         self.i = np.zeros(mem_cell_ct)
#         self.f = np.zeros(mem_cell_ct)
#         self.o = np.zeros(mem_cell_ct)
#         self.s = np.zeros(mem_cell_ct)
#         self.h = np.zeros(mem_cell_ct)
#         self.bottom_diff_h = np.zeros_like(self.h)
#         self.bottom_diff_s = np.zeros_like(self.s)
    
# class LstmNode:
#     def __init__(self, lstm_param, lstm_state):
#         # store reference to parameters and to activations
#         self.state = lstm_state
#         self.param = lstm_param
#         # non-recurrent input concatenated with recurrent input
#         self.xc = None

#     def bottom_data_is(self, x, s_prev = None, h_prev = None):
#         # if this is the first lstm node in the network
#         if s_prev is None: s_prev = np.zeros_like(self.state.s)
#         if h_prev is None: h_prev = np.zeros_like(self.state.h)
#         # save data for use in backprop
#         self.s_prev = s_prev
#         self.h_prev = h_prev

#         # concatenate x(t) and h(t-1)
#         xc = np.hstack((x,  h_prev))
#         self.state.g = np.model.activation(np.dot(self.param.wg, xc) + self.param.bg)
#         self.state.i = model.sigmoid(np.dot(self.param.wi, xc) + self.param.bi)
#         self.state.f = model.sigmoid(np.dot(self.param.wf, xc) + self.param.bf)
#         self.state.o = model.sigmoid(np.dot(self.param.wo, xc) + self.param.bo)
#         self.state.s = self.state.g * self.state.i + s_prev * self.state.f
#         self.state.h = self.state.s * self.state.o

#         self.xc = xc
    
#     def top_diff_is(self, top_diff_h, top_diff_s):
#         # notice that top_diff_s is carried along the constant error carousel
#         ds = self.state.o * top_diff_h + top_diff_s
#         do = self.state.s * top_diff_h
#         di = self.state.g * ds
#         dg = self.state.i * ds
#         df = self.s_prev * ds

#         # diffs w.r.t. vector inside sigma / model.activation function
#         di_input = model.sigmoid_derivative(self.state.i) * di 
#         df_input = model.sigmoid_derivative(self.state.f) * df 
#         do_input = model.sigmoid_derivative(self.state.o) * do 
#         dg_input = model.activation_derivative(self.state.g) * dg

#         # diffs w.r.t. inputs
#         self.param.wi_diff += np.outer(di_input, self.xc)
#         self.param.wf_diff += np.outer(df_input, self.xc)
#         self.param.wo_diff += np.outer(do_input, self.xc)
#         self.param.wg_diff += np.outer(dg_input, self.xc)
#         self.param.bi_diff += di_input
#         self.param.bf_diff += df_input       
#         self.param.bo_diff += do_input
#         self.param.bg_diff += dg_input       

#         # compute bottom diff
#         dxc = np.zeros_like(self.xc)
#         dxc += np.dot(self.param.wi.T, di_input)
#         dxc += np.dot(self.param.wf.T, df_input)
#         dxc += np.dot(self.param.wo.T, do_input)
#         dxc += np.dot(self.param.wg.T, dg_input)

#         # save bottom diffs
#         self.state.bottom_diff_s = ds * self.state.f
#         self.state.bottom_diff_h = dxc[self.param.x_dim:]

# class LstmNetwork():
#     def __init__(self, lstm_param):
#         self.lstm_param = lstm_param
#         self.lstm_node_list = []
#         # input sequence
#         self.x_list = []

#     def y_list_is(self, y_list, loss_layer):
#         """
#         Updates diffs by setting target sequence 
#         with corresponding loss layer. 
#         Will *NOT* update parameters.  To update parameters,
#         call self.lstm_param.apply_diff()
#         """
#         assert len(y_list) == len(self.x_list)
#         idx = len(self.x_list) - 1
#         # first node only gets diffs from label ...
#         loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])
#         diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])
#         # here s is not affecting loss due to h(t+1), hence we set equal to zero
#         diff_s = np.zeros(self.lstm_param.mem_cell_ct)
#         self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)
#         idx -= 1

#         ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h
#         ### we also propagate error along constant error carousel using diff_s
#         while idx >= 0:
#             loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])
#             diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])
#             diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h
#             diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s
#             self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)
#             idx -= 1 

#         return loss

#     def x_list_clear(self):
#         self.x_list = []

#     def x_list_add(self, x):
#         self.x_list.append(x)
#         if len(self.x_list) > len(self.lstm_node_list):
#             # need to add new lstm node, create new state mem
#             lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)
#             self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))

#         # get index of most recent x input
#         idx = len(self.x_list) - 1
#         if idx == 0:
#             # no recurrent inputs yet
#             self.lstm_node_list[idx].bottom_data_is(x)
#         else:
#             s_prev = self.lstm_node_list[idx - 1].state.s
#             h_prev = self.lstm_node_list[idx - 1].state.h
#             self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)







# class ToyLossLayer:
#     """
#     Computes square loss with first element of hidden layer array.
#     """
#     @classmethod
#     def loss(self, pred, label):
#         return (pred[0] - label) ** 2

#     @classmethod
#     def bottom_diff(self, pred, label):
#         diff = np.zeros_like(pred)
#         diff[0] = 2 * (pred[0] - label)
#         return diff


# # learns to repeat simple sequence from random inputs
# np.random.seed(0)

# # parameters for input data dimension and lstm cell count
# mem_cell_ct = 1000
# x_dim = 50
# lstm_param = LstmParam(mem_cell_ct, x_dim)
# lstm_net = LstmNetwork(lstm_param)
# y_list = [-0.5, 0.2, 0.1, -0.5]
# input_val_arr = [np.random.random(x_dim) for _ in y_list]

# for cur_iter in range(100):
#     print("iter", "%2s" % str(cur_iter), end=": ")
#     for ind in range(len(y_list)):
#         lstm_net.x_list_add(input_val_arr[ind])

#     print(f"y_pred = [{', '.join(['% 2.5f' % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(y_list))])}]", end=", ")

#     loss = lstm_net.y_list_is(y_list, ToyLossLayer)
#     print("loss: %.3e" % loss)
#     lstm_param.apply_diff(lr=0.1)
#     lstm_net.x_list_clear()








# # import numpy as np               #for maths
# # import pandas as pd              #for data manipulation
# # import matplotlib.pyplot as plt  #for visualization


# # #data 
# # path = r'./data/NationalNames.csv'
# # data = pd.read_csv(path)

# # #get names from the dataset
# # data['Name'] = data['Name']

# # #get first 10000 names
# # data = np.array(data['Name'][:10000]).reshape(-1,1)

# # #covert the names to lowee case
# # data = [x.lower() for x in data[:,0]]

# # data = np.array(data).reshape(-1,1)

# # print("Data Shape = {}".format(data.shape))
# # print()
# # print("Lets see some names : ")
# # print(data[1:10])

# # #to store the transform data
# # transform_data = np.copy(data)

# # #find the max length name
# # max_length = 0
# # for index in range(len(data)):
# #     max_length = max(max_length,len(data[index,0]))

# # #make every name of max length by adding '.'
# # for index in range(len(data)):
# #     length = (max_length - len(data[index,0]))
# #     string = '.'*length
# #     transform_data[index,0] = ''.join([transform_data[index,0],string])

# # print("Transformed Data")
# # print(transform_data[1:10])

# # #to store the vocabulary
# # vocab = list()
# # for name in transform_data[:,0]:
# #     vocab.extend(list(name))

# # vocab = set(vocab)
# # vocab_size = len(vocab)

# # print("Vocab size = {}".format(len(vocab)))
# # print("Vocab      = {}".format(vocab))

# # #map char to id and id to chars
# # char_id = dict()
# # id_char = dict()

# # for i,char in enumerate(vocab):
# #     char_id[char] = i
# #     id_char[i] = char

# # print('a-{}, 22-{}'.format(char_id['a'],id_char[22]))


# # # list of batches of size = 20
# # train_dataset = []

# # batch_size = 20

# # #split the trasnform data into batches of 20
# # for i in range(len(transform_data)-batch_size+1):
# #     start = i*batch_size
# #     end = start+batch_size
    
# #     #batch data
# #     batch_data = transform_data[start:end]
    
# #     if(len(batch_data)!=batch_size):
# #         break
        
# #     #convert each char of each name of batch data into one hot encoding
# #     char_list = []
# #     for k in range(len(batch_data[0][0])):
# #         batch_dataset = np.zeros([batch_size,len(vocab)])
# #         for j in range(batch_size):
# #             name = batch_data[j][0]
# #             char_index = char_id[name[k]]
# #             batch_dataset[j,char_index] = 1.0
     
# #         #store the ith char's one hot representation of each name in batch_data
# #         char_list.append(batch_dataset)
    
# #     #store each char's of every name in batch dataset into train_dataset
# #     train_dataset.append(char_list)


# # #number of input units or embedding size
# # input_units = 100

# # #number of hidden neurons
# # hidden_units = 256

# # #number of output units i.e vocab size
# # output_units = vocab_size

# # #learning rate
# # learning_rate = 0.005

# # #beta1 for V parameters used in Adam Optimizer
# # beta1 = 0.90

# # #beta2 for S parameters used in Adam Optimizer
# # beta2 = 0.99

# # #Activation Functions
# # #model.sigmoid
# # def model.sigmoid(X):
# #     return 1/(1+np.exp(-X))

# # #model.activation activation
# # def model.activation_activation(X):
# #     return np.model.activation(X)

# # #softmax activation
# # def softmax(X):
# #     exp_X = np.exp(X)
# #     exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)
# #     exp_X = exp_X/exp_X_sum
# #     return exp_X

# # #derivative of model.activation
# # def model.activation_derivative(X):
# #     return 1-(X**2)


# # #initialize parameters
# # def initialize_parameters():
# #     #initialize the parameters with 0 mean and 0.01 standard deviation
# #     mean = 0
# #     std = 0.01
    
# #     #lstm cell weights
# #     forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
    
# #     #hidden to output weights (output cell)
# #     hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))
    
# #     parameters = dict()
# #     parameters['fgw'] = forget_gate_weights
# #     parameters['igw'] = input_gate_weights
# #     parameters['ogw'] = output_gate_weights
# #     parameters['ggw'] = gate_gate_weights
# #     parameters['how'] = hidden_output_weights
    
# #     return parameters

# # #single lstm cell
# # def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ogw = parameters['ogw']
# #     ggw = parameters['ggw']
    
# #     #concat batch data and prev_activation matrix
# #     concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)
    
# #     #forget gate activations
# #     fa = np.matmul(concat_dataset,fgw)
# #     fa = model.sigmoid(fa)
    
# #     #input gate activations
# #     ia = np.matmul(concat_dataset,igw)
# #     ia = model.sigmoid(ia)
    
# #     #output gate activations
# #     oa = np.matmul(concat_dataset,ogw)
# #     oa = model.sigmoid(oa)
    
# #     #gate gate activations
# #     ga = np.matmul(concat_dataset,ggw)
# #     ga = model.activation_activation(ga)
    
# #     #new cell memory matrix
# #     cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)
    
# #     #current activation matrix
# #     activation_matrix = np.multiply(oa, model.activation_activation(cell_memory_matrix))
    
# #     #lets store the activations to be used in back prop
# #     lstm_activations = dict()
# #     lstm_activations['fa'] = fa
# #     lstm_activations['ia'] = ia
# #     lstm_activations['oa'] = oa
# #     lstm_activations['ga'] = ga
    
# #     return lstm_activations,cell_memory_matrix,activation_matrix

# # def output_cell(activation_matrix,parameters):
# #     #get hidden to output parameters
# #     how = parameters['how']
    
# #     #get outputs 
# #     output_matrix = np.matmul(activation_matrix,how)
# #     output_matrix = softmax(output_matrix)
    
# #     return output_matrix

# # def get_embeddings(batch_dataset,embeddings):
# #     embedding_dataset = np.matmul(batch_dataset,embeddings)
# #     return embedding_dataset


# # #forward propagation
# # def forward_propagation(batches,parameters,embeddings):
# #     #get batch size
# #     batch_size = batches[0].shape[0]
    
# #     #to store the activations of all the unrollings.
# #     lstm_cache = dict()                 #lstm cache
# #     activation_cache = dict()           #activation cache 
# #     cell_cache = dict()                 #cell cache
# #     output_cache = dict()               #output cache
# #     embedding_cache = dict()            #embedding cache 
    
# #     #initial activation_matrix(a0) and cell_matrix(c0)
# #     a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)
# #     c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)
    
# #     #store the initial activations in cache
# #     activation_cache['a0'] = a0
# #     cell_cache['c0'] = c0
    
# #     #unroll the names
# #     for i in range(len(batches)-1):
# #         #get first first character batch
# #         batch_dataset = batches[i]
        
# #         #get embeddings 
# #         batch_dataset = get_embeddings(batch_dataset,embeddings)
# #         embedding_cache['emb'+str(i)] = batch_dataset
        
# #         #lstm cell
# #         lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)
        
# #         #output cell
# #         ot = output_cell(at,parameters)
        
# #         #store the time 't' activations in caches
# #         lstm_cache['lstm' + str(i+1)]  = lstm_activations
# #         activation_cache['a'+str(i+1)] = at
# #         cell_cache['c' + str(i+1)] = ct
# #         output_cache['o'+str(i+1)] = ot
        
# #         #update a0 and c0 to new 'at' and 'ct' for next lstm cell
# #         a0 = at
# #         c0 = ct
        
# #     return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache

# # #calculate loss, perplexity and accuracy
# # def cal_loss_accuracy(batch_labels,output_cache):
# #     loss = 0  #to sum loss for each time step
# #     acc  = 0  #to sum acc for each time step 
# #     prob = 1  #probability product of each time step predicted char
    
# #     #batch size
# #     batch_size = batch_labels[0].shape[0]
    
# #     #loop through each time step
# #     for i in range(1,len(output_cache)+1):
# #         #get true labels and predictions
# #         labels = batch_labels[i]
# #         pred = output_cache['o'+str(i)]
        
# #         prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))
# #         loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)
# #         acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)
    
# #     #calculate perplexity loss and accuracy
# #     perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size
# #     loss = np.sum(loss)*(-1/batch_size)
# #     acc  = np.sum(acc)/(batch_size)
# #     acc = acc/len(output_cache)
    
# #     return perplexity,loss,acc

# # #calculate output cell errors
# # def calculate_output_cell_error(batch_labels,output_cache,parameters):
# #     #to store the output errors for each time step
# #     output_error_cache = dict()
# #     activation_error_cache = dict()
# #     how = parameters['how']
    
# #     #loop through each time step
# #     for i in range(1,len(output_cache)+1):
# #         #get true and predicted labels
# #         labels = batch_labels[i]
# #         pred = output_cache['o'+str(i)]
        
# #         #calculate the output_error for time step 't'
# #         error_output = pred - labels
        
# #         #calculate the activation error for time step 't'
# #         error_activation = np.matmul(error_output,how.T)
        
# #         #store the output and activation error in dict
# #         output_error_cache['eo'+str(i)] = error_output
# #         activation_error_cache['ea'+str(i)] = error_activation
        
# #     return output_error_cache,activation_error_cache

# # #calculate error for single lstm cell
# # def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):
# #     #activation error =  error coming from output cell and error coming from the next lstm cell
# #     activation_error = activation_output_error + next_activation_error
    
# #     #output gate error
# #     oa = lstm_activation['oa']
# #     eo = np.multiply(activation_error,model.activation_activation(cell_activation))
# #     eo = np.multiply(np.multiply(eo,oa),1-oa)
    
# #     #cell activation error
# #     cell_error = np.multiply(activation_error,oa)
# #     cell_error = np.multiply(cell_error,model.activation_derivative(model.activation_activation(cell_activation)))
# #     #error also coming from next lstm cell 
# #     cell_error += next_cell_error
    
# #     #input gate error
# #     ia = lstm_activation['ia']
# #     ga = lstm_activation['ga']
# #     ei = np.multiply(cell_error,ga)
# #     ei = np.multiply(np.multiply(ei,ia),1-ia)
    
# #     #gate gate error
# #     eg = np.multiply(cell_error,ia)
# #     eg = np.multiply(eg,model.activation_derivative(ga))
    
# #     #forget gate error
# #     fa = lstm_activation['fa']
# #     ef = np.multiply(cell_error,prev_cell_activation)
# #     ef = np.multiply(np.multiply(ef,fa),1-fa)
    
# #     #prev cell error
# #     prev_cell_error = np.multiply(cell_error,fa)
    
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ggw = parameters['ggw']
# #     ogw = parameters['ogw']
    
# #     #embedding + hidden activation error
# #     embed_activation_error = np.matmul(ef,fgw.T)
# #     embed_activation_error += np.matmul(ei,igw.T)
# #     embed_activation_error += np.matmul(eo,ogw.T)
# #     embed_activation_error += np.matmul(eg,ggw.T)
    
# #     input_hidden_units = fgw.shape[0]
# #     hidden_units = fgw.shape[1]
# #     input_units = input_hidden_units - hidden_units
    
# #     #prev activation error
# #     prev_activation_error = embed_activation_error[:,input_units:]
    
# #     #input error (embedding error)
# #     embed_error = embed_activation_error[:,:input_units]
    
# #     #store lstm error
# #     lstm_error = dict()
# #     lstm_error['ef'] = ef
# #     lstm_error['ei'] = ei
# #     lstm_error['eo'] = eo
# #     lstm_error['eg'] = eg
    
# #     return prev_activation_error,prev_cell_error,embed_error,lstm_error

# # #calculate output cell derivatives
# # def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):
# #     #to store the sum of derivatives from each time step
# #     dhow = np.zeros(parameters['how'].shape)
    
# #     batch_size = activation_cache['a1'].shape[0]
    
# #     #loop through the time steps 
# #     for i in range(1,len(output_error_cache)+1):
# #         #get output error
# #         output_error = output_error_cache['eo' + str(i)]
        
# #         #get input activation
# #         activation = activation_cache['a'+str(i)]
        
# #         #cal derivative and summing up!
# #         dhow += np.matmul(activation.T,output_error)/batch_size
        
# #     return dhow

# # #calculate derivatives for single lstm cell
# # def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):
# #     #get error for single time step
# #     ef = lstm_error['ef']
# #     ei = lstm_error['ei']
# #     eo = lstm_error['eo']
# #     eg = lstm_error['eg']
    
# #     #get input activations for this time step
# #     concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)
    
# #     batch_size = embedding_matrix.shape[0]
    
# #     #cal derivatives for this time step
# #     dfgw = np.matmul(concat_matrix.T,ef)/batch_size
# #     digw = np.matmul(concat_matrix.T,ei)/batch_size
# #     dogw = np.matmul(concat_matrix.T,eo)/batch_size
# #     dggw = np.matmul(concat_matrix.T,eg)/batch_size
    
# #     #store the derivatives for this time step in dict
# #     derivatives = dict()
# #     derivatives['dfgw'] = dfgw
# #     derivatives['digw'] = digw
# #     derivatives['dogw'] = dogw
# #     derivatives['dggw'] = dggw
    
# #     return derivatives

# # #backpropagation
# # def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):
# #     #calculate output errors 
# #     output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)
    
# #     #to store lstm error for each time step
# #     lstm_error_cache = dict()
    
# #     #to store embeding errors for each time step
# #     embedding_error_cache = dict()
    
# #     # next activation error 
# #     # next cell error  
# #     #for last cell will be zero
# #     eat = np.zeros(activation_error_cache['ea1'].shape)
# #     ect = np.zeros(activation_error_cache['ea1'].shape)
    
# #     #calculate all lstm cell errors (going from last time-step to the first time step)
# #     for i in range(len(lstm_cache),0,-1):
# #         #calculate the lstm errors for this time step 't'
# #         pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])
        
# #         #store the lstm error in dict
# #         lstm_error_cache['elstm'+str(i)] = le
        
# #         #store the embedding error in dict
# #         embedding_error_cache['eemb'+str(i-1)] = ee
        
# #         #update the next activation error and next cell error for previous cell
# #         eat = pae
# #         ect = pce
    
    
# #     #calculate output cell derivatives
# #     derivatives = dict()
# #     derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)
    
# #     #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict
# #     lstm_derivatives = dict()
# #     for i in range(1,len(lstm_error_cache)+1):
# #         lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])
    
# #     #initialize the derivatives to zeros 
# #     derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)
# #     derivatives['digw'] = np.zeros(parameters['igw'].shape)
# #     derivatives['dogw'] = np.zeros(parameters['ogw'].shape)
# #     derivatives['dggw'] = np.zeros(parameters['ggw'].shape)
    
# #     #sum up the derivatives for each time step
# #     for i in range(1,len(lstm_error_cache)+1):
# #         derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']
# #         derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']
# #         derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']
# #         derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']
    
# #     return derivatives,embedding_error_cache

# # #update the parameters using adam optimizer
# # #adam optimization
# # def update_parameters(parameters,derivatives,V,S,t):
# #     #get derivatives
# #     dfgw = derivatives['dfgw']
# #     digw = derivatives['digw']
# #     dogw = derivatives['dogw']
# #     dggw = derivatives['dggw']
# #     dhow = derivatives['dhow']
    
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ogw = parameters['ogw']
# #     ggw = parameters['ggw']
# #     how = parameters['how']
    
# #     #get V parameters
# #     vfgw = V['vfgw']
# #     vigw = V['vigw']
# #     vogw = V['vogw']
# #     vggw = V['vggw']
# #     vhow = V['vhow']
    
# #     #get S parameters
# #     sfgw = S['sfgw']
# #     sigw = S['sigw']
# #     sogw = S['sogw']
# #     sggw = S['sggw']
# #     show = S['show']
    
# #     #calculate the V parameters from V and current derivatives
# #     vfgw = (beta1*vfgw + (1-beta1)*dfgw)
# #     vigw = (beta1*vigw + (1-beta1)*digw)
# #     vogw = (beta1*vogw + (1-beta1)*dogw)
# #     vggw = (beta1*vggw + (1-beta1)*dggw)
# #     vhow = (beta1*vhow + (1-beta1)*dhow)
    
# #     #calculate the S parameters from S and current derivatives
# #     sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))
# #     sigw = (beta2*sigw + (1-beta2)*(digw**2))
# #     sogw = (beta2*sogw + (1-beta2)*(dogw**2))
# #     sggw = (beta2*sggw + (1-beta2)*(dggw**2))
# #     show = (beta2*show + (1-beta2)*(dhow**2))
    
# #     #update the parameters
# #     fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))
# #     igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))
# #     ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))
# #     ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))
# #     how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))
    
# #     #store the new weights
# #     parameters['fgw'] = fgw
# #     parameters['igw'] = igw
# #     parameters['ogw'] = ogw
# #     parameters['ggw'] = ggw
# #     parameters['how'] = how
    
# #     #store the new V parameters
# #     V['vfgw'] = vfgw 
# #     V['vigw'] = vigw 
# #     V['vogw'] = vogw 
# #     V['vggw'] = vggw
# #     V['vhow'] = vhow
    
# #     #store the s parameters
# #     S['sfgw'] = sfgw 
# #     S['sigw'] = sigw 
# #     S['sogw'] = sogw 
# #     S['sggw'] = sggw
# #     S['show'] = show
    
# #     return parameters,V,S    

# # def initialize_V(parameters):
# #     Vfgw = np.zeros(parameters['fgw'].shape)
# #     Vigw = np.zeros(parameters['igw'].shape)
# #     Vogw = np.zeros(parameters['ogw'].shape)
# #     Vggw = np.zeros(parameters['ggw'].shape)
# #     Vhow = np.zeros(parameters['how'].shape)
    
# #     V = dict()
# #     V['vfgw'] = Vfgw
# #     V['vigw'] = Vigw
# #     V['vogw'] = Vogw
# #     V['vggw'] = Vggw
# #     V['vhow'] = Vhow
# #     return V

# # def initialize_S(parameters):
# #     Sfgw = np.zeros(parameters['fgw'].shape)
# #     Sigw = np.zeros(parameters['igw'].shape)
# #     Sogw = np.zeros(parameters['ogw'].shape)
# #     Sggw = np.zeros(parameters['ggw'].shape)
# #     Show = np.zeros(parameters['how'].shape)
    
# #     S = dict()
# #     S['sfgw'] = Sfgw
# #     S['sigw'] = Sigw
# #     S['sogw'] = Sogw
# #     S['sggw'] = Sggw
# #     S['show'] = Show
# #     return S

# # #train function
# # def train(train_dataset,iters=1000,batch_size=20):
# #     #initalize the parameters
# #     parameters = initialize_parameters()
    
# #     #initialize the V and S parameters for Adam
# #     V = initialize_V(parameters)
# #     S = initialize_S(parameters)
    
# #     #generate the random embeddings
# #     embeddings = np.random.normal(0,0.01,(len(vocab),input_units))
    
# #     #to store the Loss, Perplexity and Accuracy for each batch
# #     J = []
# #     P = []
# #     A = []
    
    
# #     for step in range(iters):
# #         #get batch dataset
# #         index = step%len(train_dataset)
# #         batches = train_dataset[index]
        
# #         #forward propagation
# #         embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)
        
# #         #calculate the loss, perplexity and accuracy
# #         perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)
        
# #         #backward propagation
# #         derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)
        
# #         #update the parameters
# #         parameters,V,S = update_parameters(parameters,derivatives,V,S,step)
        
# #         #update the embeddings
# #         embeddings = update_embeddings(embeddings,embedding_error_cache,batches)
        
        
# #         J.append(loss)
# #         P.append(perplexity)
# #         A.append(acc)
        
# #         #print loss, accuracy and perplexity
# #         if(step%1000==0):
# #             print("For Single Batch :")
# #             print('Step       = {}'.format(step))
# #             print('Loss       = {}'.format(round(loss,2)))
# #             print('Perplexity = {}'.format(round(perplexity,2)))
# #             print('Accuracy   = {}'.format(round(acc*100,2)))
# #             print()
    
# #     return embeddings, parameters,J,P,A

# # #update the Embeddings
# # def update_embeddings(embeddings,embedding_error_cache,batch_labels):
# #     #to store the embeddings derivatives
# #     embedding_derivatives = np.zeros(embeddings.shape)
    
# #     batch_size = batch_labels[0].shape[0]
    
# #     #sum the embedding derivatives for each time step
# #     for i in range(len(embedding_error_cache)):
# #         embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size
    
# #     #update the embeddings
# #     embeddings = embeddings - learning_rate*embedding_derivatives
# #     return embeddings

# # embeddings,parameters,J,P,A = train(train_dataset,iters=8001)

# # avg_loss = list()
# # avg_acc = list()
# # avg_perp = list()
# # i = 0
# # while(i<len(J)):
# #     avg_loss.append(np.mean(J[i:i+30]))
# #     avg_acc.append(np.mean(A[i:i+30]))
# #     avg_perp.append(np.mean(P[i:i+30]))
# #     i += 30

# # plt.plot(list(range(len(avg_loss))),avg_loss)
# # plt.xlabel("x")
# # plt.ylabel("Loss (Avg of 30 batches)")
# # plt.title("Loss Graph")
# # plt.show()

# # plt.plot(list(range(len(avg_perp))),avg_perp)
# # plt.xlabel("x")
# # plt.ylabel("Perplexity (Avg of 30 batches)")
# # plt.title("Perplexity Graph")
# # plt.show()

# # plt.plot(list(range(len(avg_acc))),avg_acc)
# # plt.xlabel("x")
# # plt.ylabel("Accuracy (Avg of 30 batches)")
# # plt.title("Accuracy Graph")
# # plt.show()    


# # #predict
# # # def predict(parameters,embeddings,id_char,vocab_size):
# # #     #to store some predicted names
# # #     names = []
    
# # #     #predict 20 names
# # #     for i in range(20):
# # #         #initial activation_matrix(a0) and cell_matrix(c0)
# # #         a0 = np.zeros([1,hidden_units],dtype=np.float32)
# # #         c0 = np.zeros([1,hidden_units],dtype=np.float32)

# # #         #initalize blank name
# # #         name = ''
        
# # #         #make a batch dataset of single char
# # #         batch_dataset = np.zeros([1,vocab_size])
        
# # #         #get random start character
# # #         index = np.random.randint(0,27,1)[0]
        
# # #         #make that index 1.0
# # #         batch_dataset[0,index] = 1.0
        
# # #         #add first char to name
# # #         name += id_char[index]
        
# # #         #get char from id_char dict
# # #         char = id_char[index]
        
# # #         #loop until algo predicts '.'
# # #         while(char!='.'):
# # #             #get embeddings
# # #             batch_dataset = get_embeddings(batch_dataset,embeddings)

# # #             #lstm cell
# # #             lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)

# # #             #output cell
# # #             ot = output_cell(at,parameters)
            
# # #             #either select random.choice ot np.argmax
# # #             pred = np.random.choice(27,1,p=ot[0])[0]
            
# # #             #get predicted char index
# # #             #pred = np.argmax(ot)
                
# # #             #add char to name
# # #             name += id_char[pred]
            
# # #             char = id_char[pred]
            
# # #             #change the batch_dataset to this new predicted char
# # #             batch_dataset = np.zeros([1,vocab_size])
# # #             batch_dataset[0,pred] = 1.0

# # #             #update a0 and c0 to new 'at' and 'ct' for next lstm cell
# # #             a0 = at
# # #             c0 = ct
            
# # #         #append the predicted name to names list
# # #         names.append(name)
        
# # #     return names








# # import numpy as np
# import sys, os
# sys.path.insert(1, os.getcwd() + "./../../network") 
# from layers import LSTM


# # # generate data
# # data = open('./wonderland.txt', 'r').read()
# # chars = list(set(data))
# # len(data), vocab_size = len(data), len(chars)
# # print(f"data has {len(data)} characters, {vocab_size} unique.")
# # char_to_ix = { ch:i for i,ch in enumerate(chars) }
# # ix_to_char = { i:ch for i,ch in enumerate(chars) }

# # # hyperparameters
# # hidden_size = 100 # size of hidden layer of neurons
# # seq_length = 25 # number of steps to unroll the RNN for
# # learning_rate = 1e-1

# # # model parameters
# # Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
# # Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
# # Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
# # bh = np.zeros((hidden_size, 1)) # hidden bias
# # by = np.zeros((vocab_size, 1)) # output bias

# # def lossFun(inputs, targets, hprev):
# #   """
# #   inputs,targets are both list of integers.
# #   hprev is Hx1 array of initial hidden state
# #   returns the loss, gradients on model parameters, and last hidden state
# #   """
# #   xs, hs, ys, ps = {}, {}, {}, {}
# #   hs[-1] = np.copy(hprev)
# #   loss = 0
# #   # forward pass
# #   for t in range(len(inputs)):
# #     xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
# #     xs[t][inputs[t]] = 1
# #     hs[t] = np.model.activation(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
# #     ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
# #     ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
# #     loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
# #   # backward pass: compute gradients going backwards
# #   dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
# #   dbh, dby = np.zeros_like(bh), np.zeros_like(by)
# #   dhnext = np.zeros_like(hs[0])
# #   for t in reversed(range(len(inputs))):
# #     dy = np.copy(ps[t])
# #     dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
# #     dWhy += np.dot(dy, hs[t].T)
# #     dby += dy
# #     dh = np.dot(Why.T, dy) + dhnext # backprop into h
# #     dhraw = (1 - hs[t] * hs[t]) * dh # backprop through model.activation nonlinearity
# #     dbh += dhraw
# #     dWxh += np.dot(dhraw, xs[t].T)
# #     dWhh += np.dot(dhraw, hs[t-1].T)
# #     dhnext = np.dot(Whh.T, dhraw)
# #   for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
# #     np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients
# #   return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

# # def sample(h, seed_ix, n):
# #   """ 
# #   sample a sequence of integers from the model 
# #   h is memory state, seed_ix is seed letter for first time step
# #   """
# #   x = np.zeros((vocab_size, 1))
# #   x[seed_ix] = 1
# #   ixes = []
# #   for t in range(n):
# #     h = np.model.activation(np.dot(Wxh, x) + np.dot(Whh, h) + bh)
# #     y = np.dot(Why, h) + by
# #     p = np.exp(y) / np.sum(np.exp(y))
# #     ix = np.random.choice(range(vocab_size), p=p.ravel())
# #     x = np.zeros((vocab_size, 1))
# #     x[ix] = 1
# #     ixes.append(ix)
# #   return ixes

# # n, p = 0, 0
# # mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
# # mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad
# # smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0


# # while True:
# #   # prepare inputs (we're sweeping from left to right in steps seq_length long)
# #   if p+seq_length+1 >= len(data) or n == 0: 
# #     hprev = np.zeros((hidden_size,1)) # reset RNN memory
# #     p = 0 # go from start of data
# #   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
# #   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

# #   # sample from the model now and then
# #   if n % 100 == 0:
# #     sample_ix = sample(hprev, inputs[0], 200)
# #     txt = ''.join(ix_to_char[ix] for ix in sample_ix)
# #     print(f"----\n {txt} \n----")

# #   # forward seq_length characters through the net and fetch gradient
# #   loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
# #   smooth_loss = smooth_loss * 0.999 + loss * 0.001
# #   if n % 100 == 0: print(f"iter {n}, loss: {smooth_loss}")
  
# #   # perform parameter update with Adagrad
# #   for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], 
# #                                 [dWxh, dWhh, dWhy, dbh, dby], 
# #                                 [mWxh, mWhh, mWhy, mbh, mby]):
# #     mem += dparam * dparam
# #     param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

# #   p += seq_length # move data pointer
# #   n += 1 # iteration counter 




# import random
# import numpy as np

# def model.sigmoid(x): 
#     return 1. / (1 + np.exp(-x))

# def model.sigmoid_derivative(values): 
#     return values*(1-values)

# def model.activation_derivative(values): 
#     return 1. - values ** 2

# # createst uniform random array w/ values in [a,b) and shape args
# def rand_arr(a, b, *args): 
#     np.random.seed(0)
#     return np.random.rand(*args) * (b - a) + a

# class LstmParam:
#     def __init__(self, mem_cell_ct, x_dim):
#         self.mem_cell_ct = mem_cell_ct
#         self.x_dim = x_dim
#         concat_len = x_dim + mem_cell_ct
        
#         # weight matrices
#         self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)
#         self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len) 
#         self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)
#         self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

#         # bias terms
#         self.bg = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bi = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bf = rand_arr(-0.1, 0.1, mem_cell_ct) 
#         self.bo = rand_arr(-0.1, 0.1, mem_cell_ct) 
        
#         # diffs (derivative of loss function w.r.t. all parameters)
#         self.wg_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wi_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wf_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.wo_diff = np.zeros((mem_cell_ct, concat_len)) 
#         self.bg_diff = np.zeros(mem_cell_ct) 
#         self.bi_diff = np.zeros(mem_cell_ct) 
#         self.bf_diff = np.zeros(mem_cell_ct) 
#         self.bo_diff = np.zeros(mem_cell_ct) 

#     def apply_diff(self, lr = 1):
#         self.wg -= lr * self.wg_diff
#         self.wi -= lr * self.wi_diff
#         self.wf -= lr * self.wf_diff
#         self.wo -= lr * self.wo_diff
#         self.bg -= lr * self.bg_diff
#         self.bi -= lr * self.bi_diff
#         self.bf -= lr * self.bf_diff
#         self.bo -= lr * self.bo_diff
#         # reset diffs to zero
#         self.wg_diff = np.zeros_like(self.wg)
#         self.wi_diff = np.zeros_like(self.wi) 
#         self.wf_diff = np.zeros_like(self.wf) 
#         self.wo_diff = np.zeros_like(self.wo) 
#         self.bg_diff = np.zeros_like(self.bg)
#         self.bi_diff = np.zeros_like(self.bi) 
#         self.bf_diff = np.zeros_like(self.bf) 
#         self.bo_diff = np.zeros_like(self.bo) 

# class LstmState:
#     def __init__(self, mem_cell_ct, x_dim):
#         self.g = np.zeros(mem_cell_ct)
#         self.i = np.zeros(mem_cell_ct)
#         self.f = np.zeros(mem_cell_ct)
#         self.o = np.zeros(mem_cell_ct)
#         self.s = np.zeros(mem_cell_ct)
#         self.h = np.zeros(mem_cell_ct)
#         self.bottom_diff_h = np.zeros_like(self.h)
#         self.bottom_diff_s = np.zeros_like(self.s)
    
# class LstmNode:
#     def __init__(self, lstm_param, lstm_state):
#         # store reference to parameters and to activations
#         self.state = lstm_state
#         self.param = lstm_param
#         # non-recurrent input concatenated with recurrent input
#         self.xc = None

#     def bottom_data_is(self, x, s_prev = None, h_prev = None):
#         # if this is the first lstm node in the network
#         if s_prev is None: s_prev = np.zeros_like(self.state.s)
#         if h_prev is None: h_prev = np.zeros_like(self.state.h)
#         # save data for use in backprop
#         self.s_prev = s_prev
#         self.h_prev = h_prev

#         # concatenate x(t) and h(t-1)
#         xc = np.hstack((x,  h_prev))
#         self.state.g = np.model.activation(np.dot(self.param.wg, xc) + self.param.bg)
#         self.state.i = model.sigmoid(np.dot(self.param.wi, xc) + self.param.bi)
#         self.state.f = model.sigmoid(np.dot(self.param.wf, xc) + self.param.bf)
#         self.state.o = model.sigmoid(np.dot(self.param.wo, xc) + self.param.bo)
#         self.state.s = self.state.g * self.state.i + s_prev * self.state.f
#         self.state.h = self.state.s * self.state.o

#         self.xc = xc
    
#     def top_diff_is(self, top_diff_h, top_diff_s):
#         # notice that top_diff_s is carried along the constant error carousel
#         ds = self.state.o * top_diff_h + top_diff_s
#         do = self.state.s * top_diff_h
#         di = self.state.g * ds
#         dg = self.state.i * ds
#         df = self.s_prev * ds

#         # diffs w.r.t. vector inside sigma / model.activation function
#         di_input = model.sigmoid_derivative(self.state.i) * di 
#         df_input = model.sigmoid_derivative(self.state.f) * df 
#         do_input = model.sigmoid_derivative(self.state.o) * do 
#         dg_input = model.activation_derivative(self.state.g) * dg

#         # diffs w.r.t. inputs
#         self.param.wi_diff += np.outer(di_input, self.xc)
#         self.param.wf_diff += np.outer(df_input, self.xc)
#         self.param.wo_diff += np.outer(do_input, self.xc)
#         self.param.wg_diff += np.outer(dg_input, self.xc)
#         self.param.bi_diff += di_input
#         self.param.bf_diff += df_input       
#         self.param.bo_diff += do_input
#         self.param.bg_diff += dg_input       

#         # compute bottom diff
#         dxc = np.zeros_like(self.xc)
#         dxc += np.dot(self.param.wi.T, di_input)
#         dxc += np.dot(self.param.wf.T, df_input)
#         dxc += np.dot(self.param.wo.T, do_input)
#         dxc += np.dot(self.param.wg.T, dg_input)

#         # save bottom diffs
#         self.state.bottom_diff_s = ds * self.state.f
#         self.state.bottom_diff_h = dxc[self.param.x_dim:]

# class LstmNetwork():
#     def __init__(self, lstm_param):
#         self.lstm_param = lstm_param
#         self.lstm_node_list = []
#         # input sequence
#         self.x_list = []

#     def y_list_is(self, y_list, loss_layer):
#         """
#         Updates diffs by setting target sequence 
#         with corresponding loss layer. 
#         Will *NOT* update parameters.  To update parameters,
#         call self.lstm_param.apply_diff()
#         """
#         assert len(y_list) == len(self.x_list)
#         idx = len(self.x_list) - 1
#         # first node only gets diffs from label ...
#         loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])
#         diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])
#         # here s is not affecting loss due to h(t+1), hence we set equal to zero
#         diff_s = np.zeros(self.lstm_param.mem_cell_ct)
#         self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)
#         idx -= 1

#         ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h
#         ### we also propagate error along constant error carousel using diff_s
#         while idx >= 0:
#             loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])
#             diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])
#             diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h
#             diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s
#             self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)
#             idx -= 1 

#         return loss

#     def x_list_clear(self):
#         self.x_list = []

#     def x_list_add(self, x):
#         self.x_list.append(x)
#         if len(self.x_list) > len(self.lstm_node_list):
#             # need to add new lstm node, create new state mem
#             lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)
#             self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))

#         # get index of most recent x input
#         idx = len(self.x_list) - 1
#         if idx == 0:
#             # no recurrent inputs yet
#             self.lstm_node_list[idx].bottom_data_is(x)
#         else:
#             s_prev = self.lstm_node_list[idx - 1].state.s
#             h_prev = self.lstm_node_list[idx - 1].state.h
#             self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)







# class ToyLossLayer:
#     """
#     Computes square loss with first element of hidden layer array.
#     """
#     @classmethod
#     def loss(self, pred, label):
#         return (pred[0] - label) ** 2

#     @classmethod
#     def bottom_diff(self, pred, label):
#         diff = np.zeros_like(pred)
#         diff[0] = 2 * (pred[0] - label)
#         return diff


# # learns to repeat simple sequence from random inputs
# np.random.seed(0)

# # parameters for input data dimension and lstm cell count
# mem_cell_ct = 1000
# x_dim = 50
# lstm_param = LstmParam(mem_cell_ct, x_dim)
# lstm_net = LstmNetwork(lstm_param)
# y_list = [-0.5, 0.2, 0.1, -0.5]
# input_val_arr = [np.random.random(x_dim) for _ in y_list]

# for cur_iter in range(100):
#     print("iter", "%2s" % str(cur_iter), end=": ")
#     for ind in range(len(y_list)):
#         lstm_net.x_list_add(input_val_arr[ind])

#     print(f"y_pred = [{', '.join(['% 2.5f' % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(y_list))])}]", end=", ")

#     loss = lstm_net.y_list_is(y_list, ToyLossLayer)
#     print("loss: %.3e" % loss)
#     lstm_param.apply_diff(lr=0.1)
#     lstm_net.x_list_clear()








# # import numpy as np               #for maths
# # import pandas as pd              #for data manipulation
# # import matplotlib.pyplot as plt  #for visualization


# # #data 
# # path = r'./data/NationalNames.csv'
# # data = pd.read_csv(path)

# # #get names from the dataset
# # data['Name'] = data['Name']

# # #get first 10000 names
# # data = np.array(data['Name'][:10000]).reshape(-1,1)

# # #covert the names to lowee case
# # data = [x.lower() for x in data[:,0]]

# # data = np.array(data).reshape(-1,1)

# # print("Data Shape = {}".format(data.shape))
# # print()
# # print("Lets see some names : ")
# # print(data[1:10])

# # #to store the transform data
# # transform_data = np.copy(data)

# # #find the max length name
# # max_length = 0
# # for index in range(len(data)):
# #     max_length = max(max_length,len(data[index,0]))

# # #make every name of max length by adding '.'
# # for index in range(len(data)):
# #     length = (max_length - len(data[index,0]))
# #     string = '.'*length
# #     transform_data[index,0] = ''.join([transform_data[index,0],string])

# # print("Transformed Data")
# # print(transform_data[1:10])

# # #to store the vocabulary
# # vocab = list()
# # for name in transform_data[:,0]:
# #     vocab.extend(list(name))

# # vocab = set(vocab)
# # vocab_size = len(vocab)

# # print("Vocab size = {}".format(len(vocab)))
# # print("Vocab      = {}".format(vocab))

# # #map char to id and id to chars
# # char_id = dict()
# # id_char = dict()

# # for i,char in enumerate(vocab):
# #     char_id[char] = i
# #     id_char[i] = char

# # print('a-{}, 22-{}'.format(char_id['a'],id_char[22]))


# # # list of batches of size = 20
# # train_dataset = []

# # batch_size = 20

# # #split the trasnform data into batches of 20
# # for i in range(len(transform_data)-batch_size+1):
# #     start = i*batch_size
# #     end = start+batch_size
    
# #     #batch data
# #     batch_data = transform_data[start:end]
    
# #     if(len(batch_data)!=batch_size):
# #         break
        
# #     #convert each char of each name of batch data into one hot encoding
# #     char_list = []
# #     for k in range(len(batch_data[0][0])):
# #         batch_dataset = np.zeros([batch_size,len(vocab)])
# #         for j in range(batch_size):
# #             name = batch_data[j][0]
# #             char_index = char_id[name[k]]
# #             batch_dataset[j,char_index] = 1.0
     
# #         #store the ith char's one hot representation of each name in batch_data
# #         char_list.append(batch_dataset)
    
# #     #store each char's of every name in batch dataset into train_dataset
# #     train_dataset.append(char_list)


# # #number of input units or embedding size
# # input_units = 100

# # #number of hidden neurons
# # hidden_units = 256

# # #number of output units i.e vocab size
# # output_units = vocab_size

# # #learning rate
# # learning_rate = 0.005

# # #beta1 for V parameters used in Adam Optimizer
# # beta1 = 0.90

# # #beta2 for S parameters used in Adam Optimizer
# # beta2 = 0.99

# # #Activation Functions
# # #model.sigmoid
# # def model.sigmoid(X):
# #     return 1/(1+np.exp(-X))

# # #model.activation activation
# # def model.activation_activation(X):
# #     return np.model.activation(X)

# # #softmax activation
# # def softmax(X):
# #     exp_X = np.exp(X)
# #     exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)
# #     exp_X = exp_X/exp_X_sum
# #     return exp_X

# # #derivative of model.activation
# # def model.activation_derivative(X):
# #     return 1-(X**2)


# # #initialize parameters
# # def initialize_parameters():
# #     #initialize the parameters with 0 mean and 0.01 standard deviation
# #     mean = 0
# #     std = 0.01
    
# #     #lstm cell weights
# #     forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
# #     gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))
    
# #     #hidden to output weights (output cell)
# #     hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))
    
# #     parameters = dict()
# #     parameters['fgw'] = forget_gate_weights
# #     parameters['igw'] = input_gate_weights
# #     parameters['ogw'] = output_gate_weights
# #     parameters['ggw'] = gate_gate_weights
# #     parameters['how'] = hidden_output_weights
    
# #     return parameters

# # #single lstm cell
# # def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ogw = parameters['ogw']
# #     ggw = parameters['ggw']
    
# #     #concat batch data and prev_activation matrix
# #     concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)
    
# #     #forget gate activations
# #     fa = np.matmul(concat_dataset,fgw)
# #     fa = model.sigmoid(fa)
    
# #     #input gate activations
# #     ia = np.matmul(concat_dataset,igw)
# #     ia = model.sigmoid(ia)
    
# #     #output gate activations
# #     oa = np.matmul(concat_dataset,ogw)
# #     oa = model.sigmoid(oa)
    
# #     #gate gate activations
# #     ga = np.matmul(concat_dataset,ggw)
# #     ga = model.activation_activation(ga)
    
# #     #new cell memory matrix
# #     cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)
    
# #     #current activation matrix
# #     activation_matrix = np.multiply(oa, model.activation_activation(cell_memory_matrix))
    
# #     #lets store the activations to be used in back prop
# #     lstm_activations = dict()
# #     lstm_activations['fa'] = fa
# #     lstm_activations['ia'] = ia
# #     lstm_activations['oa'] = oa
# #     lstm_activations['ga'] = ga
    
# #     return lstm_activations,cell_memory_matrix,activation_matrix

# # def output_cell(activation_matrix,parameters):
# #     #get hidden to output parameters
# #     how = parameters['how']
    
# #     #get outputs 
# #     output_matrix = np.matmul(activation_matrix,how)
# #     output_matrix = softmax(output_matrix)
    
# #     return output_matrix

# # def get_embeddings(batch_dataset,embeddings):
# #     embedding_dataset = np.matmul(batch_dataset,embeddings)
# #     return embedding_dataset


# # #forward propagation
# # def forward_propagation(batches,parameters,embeddings):
# #     #get batch size
# #     batch_size = batches[0].shape[0]
    
# #     #to store the activations of all the unrollings.
# #     lstm_cache = dict()                 #lstm cache
# #     activation_cache = dict()           #activation cache 
# #     cell_cache = dict()                 #cell cache
# #     output_cache = dict()               #output cache
# #     embedding_cache = dict()            #embedding cache 
    
# #     #initial activation_matrix(a0) and cell_matrix(c0)
# #     a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)
# #     c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)
    
# #     #store the initial activations in cache
# #     activation_cache['a0'] = a0
# #     cell_cache['c0'] = c0
    
# #     #unroll the names
# #     for i in range(len(batches)-1):
# #         #get first first character batch
# #         batch_dataset = batches[i]
        
# #         #get embeddings 
# #         batch_dataset = get_embeddings(batch_dataset,embeddings)
# #         embedding_cache['emb'+str(i)] = batch_dataset
        
# #         #lstm cell
# #         lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)
        
# #         #output cell
# #         ot = output_cell(at,parameters)
        
# #         #store the time 't' activations in caches
# #         lstm_cache['lstm' + str(i+1)]  = lstm_activations
# #         activation_cache['a'+str(i+1)] = at
# #         cell_cache['c' + str(i+1)] = ct
# #         output_cache['o'+str(i+1)] = ot
        
# #         #update a0 and c0 to new 'at' and 'ct' for next lstm cell
# #         a0 = at
# #         c0 = ct
        
# #     return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache

# # #calculate loss, perplexity and accuracy
# # def cal_loss_accuracy(batch_labels,output_cache):
# #     loss = 0  #to sum loss for each time step
# #     acc  = 0  #to sum acc for each time step 
# #     prob = 1  #probability product of each time step predicted char
    
# #     #batch size
# #     batch_size = batch_labels[0].shape[0]
    
# #     #loop through each time step
# #     for i in range(1,len(output_cache)+1):
# #         #get true labels and predictions
# #         labels = batch_labels[i]
# #         pred = output_cache['o'+str(i)]
        
# #         prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))
# #         loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)
# #         acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)
    
# #     #calculate perplexity loss and accuracy
# #     perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size
# #     loss = np.sum(loss)*(-1/batch_size)
# #     acc  = np.sum(acc)/(batch_size)
# #     acc = acc/len(output_cache)
    
# #     return perplexity,loss,acc

# # #calculate output cell errors
# # def calculate_output_cell_error(batch_labels,output_cache,parameters):
# #     #to store the output errors for each time step
# #     output_error_cache = dict()
# #     activation_error_cache = dict()
# #     how = parameters['how']
    
# #     #loop through each time step
# #     for i in range(1,len(output_cache)+1):
# #         #get true and predicted labels
# #         labels = batch_labels[i]
# #         pred = output_cache['o'+str(i)]
        
# #         #calculate the output_error for time step 't'
# #         error_output = pred - labels
        
# #         #calculate the activation error for time step 't'
# #         error_activation = np.matmul(error_output,how.T)
        
# #         #store the output and activation error in dict
# #         output_error_cache['eo'+str(i)] = error_output
# #         activation_error_cache['ea'+str(i)] = error_activation
        
# #     return output_error_cache,activation_error_cache

# # #calculate error for single lstm cell
# # def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):
# #     #activation error =  error coming from output cell and error coming from the next lstm cell
# #     activation_error = activation_output_error + next_activation_error
    
# #     #output gate error
# #     oa = lstm_activation['oa']
# #     eo = np.multiply(activation_error,model.activation_activation(cell_activation))
# #     eo = np.multiply(np.multiply(eo,oa),1-oa)
    
# #     #cell activation error
# #     cell_error = np.multiply(activation_error,oa)
# #     cell_error = np.multiply(cell_error,model.activation_derivative(model.activation_activation(cell_activation)))
# #     #error also coming from next lstm cell 
# #     cell_error += next_cell_error
    
# #     #input gate error
# #     ia = lstm_activation['ia']
# #     ga = lstm_activation['ga']
# #     ei = np.multiply(cell_error,ga)
# #     ei = np.multiply(np.multiply(ei,ia),1-ia)
    
# #     #gate gate error
# #     eg = np.multiply(cell_error,ia)
# #     eg = np.multiply(eg,model.activation_derivative(ga))
    
# #     #forget gate error
# #     fa = lstm_activation['fa']
# #     ef = np.multiply(cell_error,prev_cell_activation)
# #     ef = np.multiply(np.multiply(ef,fa),1-fa)
    
# #     #prev cell error
# #     prev_cell_error = np.multiply(cell_error,fa)
    
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ggw = parameters['ggw']
# #     ogw = parameters['ogw']
    
# #     #embedding + hidden activation error
# #     embed_activation_error = np.matmul(ef,fgw.T)
# #     embed_activation_error += np.matmul(ei,igw.T)
# #     embed_activation_error += np.matmul(eo,ogw.T)
# #     embed_activation_error += np.matmul(eg,ggw.T)
    
# #     input_hidden_units = fgw.shape[0]
# #     hidden_units = fgw.shape[1]
# #     input_units = input_hidden_units - hidden_units
    
# #     #prev activation error
# #     prev_activation_error = embed_activation_error[:,input_units:]
    
# #     #input error (embedding error)
# #     embed_error = embed_activation_error[:,:input_units]
    
# #     #store lstm error
# #     lstm_error = dict()
# #     lstm_error['ef'] = ef
# #     lstm_error['ei'] = ei
# #     lstm_error['eo'] = eo
# #     lstm_error['eg'] = eg
    
# #     return prev_activation_error,prev_cell_error,embed_error,lstm_error

# # #calculate output cell derivatives
# # def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):
# #     #to store the sum of derivatives from each time step
# #     dhow = np.zeros(parameters['how'].shape)
    
# #     batch_size = activation_cache['a1'].shape[0]
    
# #     #loop through the time steps 
# #     for i in range(1,len(output_error_cache)+1):
# #         #get output error
# #         output_error = output_error_cache['eo' + str(i)]
        
# #         #get input activation
# #         activation = activation_cache['a'+str(i)]
        
# #         #cal derivative and summing up!
# #         dhow += np.matmul(activation.T,output_error)/batch_size
        
# #     return dhow

# # #calculate derivatives for single lstm cell
# # def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):
# #     #get error for single time step
# #     ef = lstm_error['ef']
# #     ei = lstm_error['ei']
# #     eo = lstm_error['eo']
# #     eg = lstm_error['eg']
    
# #     #get input activations for this time step
# #     concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)
    
# #     batch_size = embedding_matrix.shape[0]
    
# #     #cal derivatives for this time step
# #     dfgw = np.matmul(concat_matrix.T,ef)/batch_size
# #     digw = np.matmul(concat_matrix.T,ei)/batch_size
# #     dogw = np.matmul(concat_matrix.T,eo)/batch_size
# #     dggw = np.matmul(concat_matrix.T,eg)/batch_size
    
# #     #store the derivatives for this time step in dict
# #     derivatives = dict()
# #     derivatives['dfgw'] = dfgw
# #     derivatives['digw'] = digw
# #     derivatives['dogw'] = dogw
# #     derivatives['dggw'] = dggw
    
# #     return derivatives

# # #backpropagation
# # def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):
# #     #calculate output errors 
# #     output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)
    
# #     #to store lstm error for each time step
# #     lstm_error_cache = dict()
    
# #     #to store embeding errors for each time step
# #     embedding_error_cache = dict()
    
# #     # next activation error 
# #     # next cell error  
# #     #for last cell will be zero
# #     eat = np.zeros(activation_error_cache['ea1'].shape)
# #     ect = np.zeros(activation_error_cache['ea1'].shape)
    
# #     #calculate all lstm cell errors (going from last time-step to the first time step)
# #     for i in range(len(lstm_cache),0,-1):
# #         #calculate the lstm errors for this time step 't'
# #         pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])
        
# #         #store the lstm error in dict
# #         lstm_error_cache['elstm'+str(i)] = le
        
# #         #store the embedding error in dict
# #         embedding_error_cache['eemb'+str(i-1)] = ee
        
# #         #update the next activation error and next cell error for previous cell
# #         eat = pae
# #         ect = pce
    
    
# #     #calculate output cell derivatives
# #     derivatives = dict()
# #     derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)
    
# #     #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict
# #     lstm_derivatives = dict()
# #     for i in range(1,len(lstm_error_cache)+1):
# #         lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])
    
# #     #initialize the derivatives to zeros 
# #     derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)
# #     derivatives['digw'] = np.zeros(parameters['igw'].shape)
# #     derivatives['dogw'] = np.zeros(parameters['ogw'].shape)
# #     derivatives['dggw'] = np.zeros(parameters['ggw'].shape)
    
# #     #sum up the derivatives for each time step
# #     for i in range(1,len(lstm_error_cache)+1):
# #         derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']
# #         derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']
# #         derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']
# #         derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']
    
# #     return derivatives,embedding_error_cache

# # #update the parameters using adam optimizer
# # #adam optimization
# # def update_parameters(parameters,derivatives,V,S,t):
# #     #get derivatives
# #     dfgw = derivatives['dfgw']
# #     digw = derivatives['digw']
# #     dogw = derivatives['dogw']
# #     dggw = derivatives['dggw']
# #     dhow = derivatives['dhow']
    
# #     #get parameters
# #     fgw = parameters['fgw']
# #     igw = parameters['igw']
# #     ogw = parameters['ogw']
# #     ggw = parameters['ggw']
# #     how = parameters['how']
    
# #     #get V parameters
# #     vfgw = V['vfgw']
# #     vigw = V['vigw']
# #     vogw = V['vogw']
# #     vggw = V['vggw']
# #     vhow = V['vhow']
    
# #     #get S parameters
# #     sfgw = S['sfgw']
# #     sigw = S['sigw']
# #     sogw = S['sogw']
# #     sggw = S['sggw']
# #     show = S['show']
    
# #     #calculate the V parameters from V and current derivatives
# #     vfgw = (beta1*vfgw + (1-beta1)*dfgw)
# #     vigw = (beta1*vigw + (1-beta1)*digw)
# #     vogw = (beta1*vogw + (1-beta1)*dogw)
# #     vggw = (beta1*vggw + (1-beta1)*dggw)
# #     vhow = (beta1*vhow + (1-beta1)*dhow)
    
# #     #calculate the S parameters from S and current derivatives
# #     sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))
# #     sigw = (beta2*sigw + (1-beta2)*(digw**2))
# #     sogw = (beta2*sogw + (1-beta2)*(dogw**2))
# #     sggw = (beta2*sggw + (1-beta2)*(dggw**2))
# #     show = (beta2*show + (1-beta2)*(dhow**2))
    
# #     #update the parameters
# #     fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))
# #     igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))
# #     ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))
# #     ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))
# #     how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))
    
# #     #store the new weights
# #     parameters['fgw'] = fgw
# #     parameters['igw'] = igw
# #     parameters['ogw'] = ogw
# #     parameters['ggw'] = ggw
# #     parameters['how'] = how
    
# #     #store the new V parameters
# #     V['vfgw'] = vfgw 
# #     V['vigw'] = vigw 
# #     V['vogw'] = vogw 
# #     V['vggw'] = vggw
# #     V['vhow'] = vhow
    
# #     #store the s parameters
# #     S['sfgw'] = sfgw 
# #     S['sigw'] = sigw 
# #     S['sogw'] = sogw 
# #     S['sggw'] = sggw
# #     S['show'] = show
    
# #     return parameters,V,S    

# # def initialize_V(parameters):
# #     Vfgw = np.zeros(parameters['fgw'].shape)
# #     Vigw = np.zeros(parameters['igw'].shape)
# #     Vogw = np.zeros(parameters['ogw'].shape)
# #     Vggw = np.zeros(parameters['ggw'].shape)
# #     Vhow = np.zeros(parameters['how'].shape)
    
# #     V = dict()
# #     V['vfgw'] = Vfgw
# #     V['vigw'] = Vigw
# #     V['vogw'] = Vogw
# #     V['vggw'] = Vggw
# #     V['vhow'] = Vhow
# #     return V

# # def initialize_S(parameters):
# #     Sfgw = np.zeros(parameters['fgw'].shape)
# #     Sigw = np.zeros(parameters['igw'].shape)
# #     Sogw = np.zeros(parameters['ogw'].shape)
# #     Sggw = np.zeros(parameters['ggw'].shape)
# #     Show = np.zeros(parameters['how'].shape)
    
# #     S = dict()
# #     S['sfgw'] = Sfgw
# #     S['sigw'] = Sigw
# #     S['sogw'] = Sogw
# #     S['sggw'] = Sggw
# #     S['show'] = Show
# #     return S

# # #train function
# # def train(train_dataset,iters=1000,batch_size=20):
# #     #initalize the parameters
# #     parameters = initialize_parameters()
    
# #     #initialize the V and S parameters for Adam
# #     V = initialize_V(parameters)
# #     S = initialize_S(parameters)
    
# #     #generate the random embeddings
# #     embeddings = np.random.normal(0,0.01,(len(vocab),input_units))
    
# #     #to store the Loss, Perplexity and Accuracy for each batch
# #     J = []
# #     P = []
# #     A = []
    
    
# #     for step in range(iters):
# #         #get batch dataset
# #         index = step%len(train_dataset)
# #         batches = train_dataset[index]
        
# #         #forward propagation
# #         embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)
        
# #         #calculate the loss, perplexity and accuracy
# #         perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)
        
# #         #backward propagation
# #         derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)
        
# #         #update the parameters
# #         parameters,V,S = update_parameters(parameters,derivatives,V,S,step)
        
# #         #update the embeddings
# #         embeddings = update_embeddings(embeddings,embedding_error_cache,batches)
        
        
# #         J.append(loss)
# #         P.append(perplexity)
# #         A.append(acc)
        
# #         #print loss, accuracy and perplexity
# #         if(step%1000==0):
# #             print("For Single Batch :")
# #             print('Step       = {}'.format(step))
# #             print('Loss       = {}'.format(round(loss,2)))
# #             print('Perplexity = {}'.format(round(perplexity,2)))
# #             print('Accuracy   = {}'.format(round(acc*100,2)))
# #             print()
    
# #     return embeddings, parameters,J,P,A

# # #update the Embeddings
# # def update_embeddings(embeddings,embedding_error_cache,batch_labels):
# #     #to store the embeddings derivatives
# #     embedding_derivatives = np.zeros(embeddings.shape)
    
# #     batch_size = batch_labels[0].shape[0]
    
# #     #sum the embedding derivatives for each time step
# #     for i in range(len(embedding_error_cache)):
# #         embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size
    
# #     #update the embeddings
# #     embeddings = embeddings - learning_rate*embedding_derivatives
# #     return embeddings

# # embeddings,parameters,J,P,A = train(train_dataset,iters=8001)

# # avg_loss = list()
# # avg_acc = list()
# # avg_perp = list()
# # i = 0
# # while(i<len(J)):
# #     avg_loss.append(np.mean(J[i:i+30]))
# #     avg_acc.append(np.mean(A[i:i+30]))
# #     avg_perp.append(np.mean(P[i:i+30]))
# #     i += 30

# # plt.plot(list(range(len(avg_loss))),avg_loss)
# # plt.xlabel("x")
# # plt.ylabel("Loss (Avg of 30 batches)")
# # plt.title("Loss Graph")
# # plt.show()

# # plt.plot(list(range(len(avg_perp))),avg_perp)
# # plt.xlabel("x")
# # plt.ylabel("Perplexity (Avg of 30 batches)")
# # plt.title("Perplexity Graph")
# # plt.show()

# # plt.plot(list(range(len(avg_acc))),avg_acc)
# # plt.xlabel("x")
# # plt.ylabel("Accuracy (Avg of 30 batches)")
# # plt.title("Accuracy Graph")
# # plt.show()    


# # #predict
# # # def predict(parameters,embeddings,id_char,vocab_size):
# # #     #to store some predicted names
# # #     names = []
    
# # #     #predict 20 names
# # #     for i in range(20):
# # #         #initial activation_matrix(a0) and cell_matrix(c0)
# # #         a0 = np.zeros([1,hidden_units],dtype=np.float32)
# # #         c0 = np.zeros([1,hidden_units],dtype=np.float32)

# # #         #initalize blank name
# # #         name = ''
        
# # #         #make a batch dataset of single char
# # #         batch_dataset = np.zeros([1,vocab_size])
        
# # #         #get random start character
# # #         index = np.random.randint(0,27,1)[0]
        
# # #         #make that index 1.0
# # #         batch_dataset[0,index] = 1.0
        
# # #         #add first char to name
# # #         name += id_char[index]
        
# # #         #get char from id_char dict
# # #         char = id_char[index]
        
# # #         #loop until algo predicts '.'
# # #         while(char!='.'):
# # #             #get embeddings
# # #             batch_dataset = get_embeddings(batch_dataset,embeddings)

# # #             #lstm cell
# # #             lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)

# # #             #output cell
# # #             ot = output_cell(at,parameters)
            
# # #             #either select random.choice ot np.argmax
# # #             pred = np.random.choice(27,1,p=ot[0])[0]
            
# # #             #get predicted char index
# # #             #pred = np.argmax(ot)
                
# # #             #add char to name
# # #             name += id_char[pred]
            
# # #             char = id_char[pred]
            
# # #             #change the batch_dataset to this new predicted char
# # #             batch_dataset = np.zeros([1,vocab_size])
# # #             batch_dataset[0,pred] = 1.0

# # #             #update a0 and c0 to new 'at' and 'ct' for next lstm cell
# # #             a0 = at
# # #             c0 = ct
            
# # #         #append the predicted name to names list
# # #         names.append(name)
        
# # #     return names


import numpy as np
import random, io, sys, os

sys.path.insert(1, os.getcwd() + "./../../network") 
import layers

# functions
from algorithms.activation_functions import act_functions 
from algorithms.loss_functions import loss_functions 
from algorithms.optimizer_functions import opt_functions


network = layers.Network2(loss="MSE", activation_output="softmax")
model = layers.LSTM(n_units=123, input_shape=(123, 23))#testing

# GRADED FUNCTION: lstm_cell_forward
forget = layers.Gate(model.optimizer)
input = layers.Gate(model.optimizer)
cell = layers.Gate(model.optimizer)
output = layers.Gate(model.optimizer)
state = layers.Gate(model.optimizer)

def lstm_cell_forward(xt, a_prev, c_prev):
    # Retrieve dimensions from shapes of xt and Wy
    n_x, m = xt.shape
    n_y, n_a = state.W.shape

    # Concatenate a_prev and xt (≈3 lines)
    X = np.zeros((n_a + n_x, m))
    X[: n_a, :] = a_prev
    X[n_a :, :] = xt

    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)
    forget.values = model.sigmoid(np.dot(forget.W, X) + forget.b)
    input.values = model.sigmoid(np.dot(input.W, X) + input.b)
    output.values = model.sigmoid(np.dot(output.W, X) + output.b)
    ct_hat = model.activation(np.dot(cell.W, X) + cell.b)
    cell.values = forget.values * c_prev + input.values * ct_hat 
    state.values = output.values * model.activation(cell.values)
    
    # Compute prediction of the LSTM cell (≈1 line)
    yt_pred = model.activation_output(np.dot(gate.W, state.values) + gate.b)

    # store values needed for backward propagation in cache
    cache = (a_prev, c_prev, ct_hat, xt)
    return yt_pred, cache

np.random.seed(1)
xt = np.random.randn(3,10)
a_prev = np.random.randn(5,10)
c_prev = np.random.randn(5,10)

forget.initialize((5, 5+3))
input.initialize((5, 5+3))
output.initialize((5, 5+3))
cell.initialize((5, 5+3))
state.initialize((2,5))

# a_next, c_next, 
# print("a_next[4] = ", a_next[4])
# print("a_next.shape = ", c_next.shape)
# print("c_next[2] = ", c_next[2])
# print("c_next.shape = ", c_next.shape)
yt, cache = lstm_cell_forward(xt, a_prev, c_prev)
print("yt[1] =", yt[1])
print("yt.shape = ", yt.shape)
print("cache[1][3] =", cache[1][3])
print("len(cache) = ", len(cache))

# GRADED FUNCTION: lstm_forward
def lstm_forward(x, a0, parameters):

    # Initialize "caches", which will track the list of all the caches
    caches = []
    
    ### START CODE HERE ###
    # Retrieve dimensions from shapes of x and Wy (≈2 lines)
    n_x, m, timesteps = x.shape
    n_y, n_a = state.W.shape
    
    # initialize "a", "c" and "y" with zeros (≈3 lines)
    a = np.zeros((n_a, m, timesteps))
    c = a
    y = np.zeros((n_y, m, timesteps))
    
    # Initialize a_next and c_next (≈2 lines)
    a_next = a0
    c_next = np.zeros(a_next.shape)
    
    # loop over all time-steps
    for t in range(timesteps):
        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)
        # a_next, c_next, 
        yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)
        # Save the value of the new "next" hidden state in a (≈1 line)
        a[:,:,t] = state.valuesa_next
        # Save the value of the prediction in y (≈1 line)
        y[:,:,t] = yt
        # Save the value of the next cell state (≈1 line)
        c[:,:,t]  = c_next
        # Append the cache into caches (≈1 line)
        caches.append(cache)
    
    # store values needed for backward propagation in cache
    caches = (caches, x)

    return a, y, c, caches

np.random.seed(1)
x = np.random.randn(3,10,7)
a0 = np.random.randn(5,10)
# Wf = np.random.randn(5, 5+3)
# bf = np.random.randn(5,1)
# Wi = np.random.randn(5, 5+3)
# bi = np.random.randn(5,1)
# Wo = np.random.randn(5, 5+3)
# bo = np.random.randn(5,1)
# Wc = np.random.randn(5, 5+3)
# bc = np.random.randn(5,1)
# Wy = np.random.randn(2,5)
# by = np.random.randn(2,1)

# parameters = {"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by}

a, y, c, caches = lstm_forward(x, a0)#, parameters)
print("a[4][3][6] = ", a[4][3][6])
print("a.shape = ", a.shape)
print("y[1][4][3] =", y[1][4][3])
print("y.shape = ", y.shape)
print("caches[1][1[1]] =", caches[1][1][1])
print("c[1][2][1]", c[1][2][1])
print("len(caches) = ", len(caches))

# def lstm_cell_backward(da_next, dc_next, cache):
#     """
#     Implement the backward pass for the LSTM-cell (single time-step).

#     Arguments:
#     da_next -- Gradients of next hidden state, of shape (n_a, m)
#     dc_next -- Gradients of next cell state, of shape (n_a, m)
#     cache -- cache storing information from the forward pass

#     Returns:
#     gradients -- python dictionary containing:
#                         dxt -- Gradient of input data at time-step t, of shape (n_x, m)
#                         da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)
#                         dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, timesteps)
#                         dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
#                         dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
#                         dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)
#                         dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)
#                         dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)
#                         dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)
#                         dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)
#                         dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)
#     """

#     # Retrieve information from "cache"
#     (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache
    
#     ### START CODE HERE ###
    
#     # # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)
#     # ft = model.sigmoid(np.dot(Wf, concat) + bf)
#     # it = model.sigmoid(np.dot(Wi, concat) + bi)
#     # cct = np.tanh(np.dot(Wc, concat) + bc)
#     # c_next = ft * c_prev + it * cct 
#     # ot = model.sigmoid(np.dot(Wo, concat) + bo)
#     # a_next = ot * np.tanh(c_next)
    
#     # # Compute prediction of the LSTM cell (≈1 line)
#     # yt_pred = model.activation_output(np.dot(Wy, a_next) + by)



#     # Retrieve dimensions from xt's and a_next's shape (≈2 lines)
#     n_x, m = xt.shape
#     n_a, m = a_next.shape
    
#     # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)
#     dot = da_next * np.tanh(c_next) * ot * (1 - ot)
#     dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))
#     dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)
#     dft = (dc_next * c_prev + ot *(1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)
    
#     # Code equations (7) to (10) (≈4 lines)
#     ##dit = None
#     ##dft = None
#     ##dot = None
#     ##dcct = None
#     concat = np.concatenate((a_prev, xt), axis=0)

#     # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)
#     dWf = np.dot(dft, concat.T)
#     dWi = np.dot(dit, concat.T)
#     dWc = np.dot(dcct, concat.T)
#     dWo = np.dot(dot, concat.T)
#     dbf = np.sum(dft, axis=1 ,keepdims = True)
#     dbi = np.sum(dit, axis=1, keepdims = True)
#     dbc = np.sum(dcct, axis=1,  keepdims = True)
#     dbo = np.sum(dot, axis=1, keepdims = True)

#     # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)
#     da_prev = np.dot(parameters['Wf'][:, :n_a].T, dft) + np.dot(parameters['Wi'][:, :n_a].T, dit) + np.dot(parameters['Wc'][:, :n_a].T, dcct) + np.dot(parameters['Wo'][:, :n_a].T, dot)
#     dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next
#     dxt = np.dot(parameters['Wf'][:, n_a:].T, dft) + np.dot(parameters['Wi'][:, n_a:].T, dit) + np.dot(parameters['Wc'][:, n_a:].T, dcct) + np.dot(parameters['Wo'][:, n_a:].T, dot)
#     ### END CODE HERE ###
    
#     # Save gradients in dictionary
#     gradients = {"dxt": dxt, "da_prev": da_prev, "dc_prev": dc_prev, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi,
#                 "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo}

#     return gradients

# np.random.seed(1)
# xt = np.random.randn(3,10)
# a_prev = np.random.randn(5,10)
# c_prev = np.random.randn(5,10)

# Wf = np.random.randn(5, 5+3)
# bf = np.random.randn(5,1)

# Wi = np.random.randn(5, 5+3)
# bi = np.random.randn(5,1)

# Wo = np.random.randn(5, 5+3)
# bo = np.random.randn(5,1)

# Wc = np.random.randn(5, 5+3)
# bc = np.random.randn(5,1)

# Wy = np.random.randn(2,5)
# by = np.random.randn(2,1)

# parameters = {"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by}

# a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)

# da_next = np.random.randn(5,10)
# dc_next = np.random.randn(5,10)
# gradients = lstm_cell_backward(da_next, dc_next, cache)
# print("gradients[\"dxt\"][1][2] =", gradients["dxt"][1][2])
# print("gradients[\"dxt\"].shape =", gradients["dxt"].shape)
# print("gradients[\"da_prev\"][2][3] =", gradients["da_prev"][2][3])
# print("gradients[\"da_prev\"].shape =", gradients["da_prev"].shape)
# print("gradients[\"dc_prev\"][2][3] =", gradients["dc_prev"][2][3])
# print("gradients[\"dc_prev\"].shape =", gradients["dc_prev"].shape)
# print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])
# print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)
# print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])
# print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)
# print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])
# print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)
# print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])
# print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)
# print("gradients[\"dbf\"][4] =", gradients["dbf"][4])
# print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)
# print("gradients[\"dbi\"][4] =", gradients["dbi"][4])
# print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)
# print("gradients[\"dbc\"][4] =", gradients["dbc"][4])
# print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)
# print("gradients[\"dbo\"][4] =", gradients["dbo"][4])
# print("gradients[\"dbo\"].shape =", gradients["dbo"].shape)

# def lstm_backward(da, caches):
    
#     """
#     Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).

#     Arguments:
#     da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)
#     dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)
#     caches -- cache storing information from the forward pass (lstm_forward)

#     Returns:
#     gradients -- python dictionary containing:
#                         dx -- Gradient of inputs, of shape (n_x, m, T_x)
#                         da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)
#                         dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
#                         dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
#                         dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)
#                         dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)
#                         dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)
#                         dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)
#                         dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)
#                         dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)
#     """

#     # Retrieve values from the first cache (t=1) of caches.
#     (caches, x) = caches
#     (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]
    
#     ### START CODE HERE ###
#     # Retrieve dimensions from da's and x1's shapes (≈2 lines)
#     n_a, m, T_x = da.shape
#     n_x, m = x1.shape
    
#     # initialize the gradients with the right sizes (≈12 lines)
#     dx = np.zeros((n_x, m, T_x))
#     da0 = np.zeros((n_a, m))
#     da_prevt = np.zeros(da0.shape)
#     dc_prevt = np.zeros(da0.shape)
#     dWf = np.zeros((n_a, n_a + n_x))
#     dWi = np.zeros(dWf.shape)
#     dWc = np.zeros(dWf.shape)
#     dWo = np.zeros(dWf.shape)
#     dbf = np.zeros((n_a, 1))
#     dbi = np.zeros(dbf.shape)
#     dbc = np.zeros(dbf.shape)
#     dbo = np.zeros(dbf.shape)
    
#     # loop back over the whole sequence
#     for t in reversed(range(T_x)):
#         # Compute all gradients using lstm_cell_backward
#         gradients = lstm_cell_backward(da[:, :, t], dc_prevt, caches[t])
#         # Store or add the gradient to the parameters' previous step's gradient
#         dx[:,:,t] = gradients["dxt"]
#         dWf += gradients["dWf"]
#         dWi += gradients["dWi"]
#         dWc += gradients["dWc"]
#         dWo += gradients["dWo"]
#         dbf += gradients["dbf"]
#         dbi += gradients["dbi"]
#         dbc += gradients["dbc"]
#         dbo += gradients["dbo"]
#     # Set the first activation's gradient to the backpropagated gradient da_prev.
#     da0 = gradients["da_prev"]
    
#     ### END CODE HERE ###

#     # Store the gradients in a python dictionary
#     gradients = {"dx": dx, "da0": da0, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi,
#                 "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo}
    
#     return gradients

# np.random.seed(1)
# x = np.random.randn(3,10,7)
# a0 = np.random.randn(5,10)
# Wf = np.random.randn(5, 5+3)
# bf = np.random.randn(5,1)
# Wi = np.random.randn(5, 5+3)
# bi = np.random.randn(5,1)
# Wo = np.random.randn(5, 5+3)
# bo = np.random.randn(5,1)
# Wc = np.random.randn(5, 5+3)
# bc = np.random.randn(5,1)

# parameters = {"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by}

# a, y, c, caches = lstm_forward(x, a0, parameters)

# da = np.random.randn(5, 10, 4)
# gradients = lstm_backward(da, caches)

# print("gradients[\"dx\"][1][2] =", gradients["dx"][1][2])
# print("gradients[\"dx\"].shape =", gradients["dx"].shape)
# print("gradients[\"da0\"][2][3] =", gradients["da0"][2][3])
# print("gradients[\"da0\"].shape =", gradients["da0"].shape)
# print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])
# print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)
# print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])
# print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)
# print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])
# print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)
# print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])
# print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)
# print("gradients[\"dbf\"][4] =", gradients["dbf"][4])
# print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)
# print("gradients[\"dbi\"][4] =", gradients["dbi"][4])
# print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)
# print("gradients[\"dbc\"][4] =", gradients["dbc"][4])
# print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)
# print("gradients[\"dbo\"][4] =", gradients["dbo"][4])
# print("gradients[\"dbo\"].shape =", gradients["dbo"].shape)



        # # y_pred = np.zeros_like(X)
                # outputs = np.zeros_like(self.output) 

                # # Forward pass, through time steps
                # for t in range(timesteps):
                #     # z = np.row_stack((self.state.values[t - 1], X[t]))

                #     self.forget.values[:, t] = self.sigmoid(
                #         X[:, t].dot(self.forget.W.T) + self.hidden.values[:, t-1].dot(self.forget.U)# + self.forget.b
                #     )
                #     self.input.values[:, t] = self.sigmoid(
                #         X[:, t].dot(self.input.W.T) + self.hidden.values[:, t-1].dot(self.input.U)# + self.input.b
                #     )
                #     self.output.values[:, t] = self.sigmoid(
                #         X[:, t].dot(self.output.W.T) + self.hidden.values[:, t-1].dot(self.output.U)# + self.output.b
                #     )
                #     cell_hat = self.activation(
                #         X[:, t].dot(self.cell.W.T) + self.hidden.values[:, t-1].dot(self.cell.U)# + self.cell.b
                #     )

                #     self.cell.values[:, t] = self.forget.values[:, t] * self.cell.values[:, t-1] + self.input.values[:, t] * cell_hat
                #     self.hidden.values[:, t] = self.output.values[:, t] * self.activation(self.cell.values[:, t])

                #     # flatten ht's
                #     outputs[:, t] = self.hidden.values[:, t].dot(self.hidden.W.T)# np.dot(self.hidden.W, self.hidden.values[:, t])# + self.hidden.b
                #     # outputs[t] = self.activation_output(outputs[:, t])# softmax (TODO remove in future when more layers)

                # return outputs



                # y_hat
                # return self.output.values# self.activation_output()# with softmax

            # def rnn_cell_backward(self, da_next, cache):
            #     (a_next, a_prev, xt, parameters) = cache
            #     Wax = parameters["Wax"]
            #     Waa = parameters["Waa"]
            #     Wya= parameters["Wya"]
            #     ba= parameters["ba"]
            #     by= parameters["by"]
            #     dtanh = (1 - a_next * a_next) * da_next
            #     dWax = np.dot(dtanh, xt.T)
            #     dxt = np.dot(Wax.T, dtanh)
            #     dWaa = np.dot(dtanh, a_prev.T)
            #     da_prev = np.dot(Waa.T, dtanh)
            #     dba = np.sum(dtanh, keepdims=True, axis=-1)
            #     gradients = {"dxt": dxt, "da_prev": da_prev,"dWax": dWax, "dWaa": dWaa, "dba": dba}
            #     return gradients




#     def top_diff_is(self, top_diff_h, top_diff_s):
#         # notice that top_diff_s is carried along the constant error carousel
#         ds = self.state.o * top_diff_h + top_diff_s
#         do = self.state.s * top_diff_h
#         di = self.state.g * ds
#         dg = self.state.i * ds
#         df = self.s_prev * ds

#         # diffs w.r.t. vector inside sigma / model.activation function
#         di_input = model.sigmoid_derivative(self.state.i) * di 
#         df_input = model.sigmoid_derivative(self.state.f) * df 
#         do_input = model.sigmoid_derivative(self.state.o) * do 
#         dg_input = model.activation_derivative(self.state.g) * dg

#         # diffs w.r.t. inputs
#         self.param.wi_diff += np.outer(di_input, self.xc)
#         self.param.wf_diff += np.outer(df_input, self.xc)
#         self.param.wo_diff += np.outer(do_input, self.xc)
#         self.param.wg_diff += np.outer(dg_input, self.xc)
#         self.param.bi_diff += di_input
#         self.param.bf_diff += df_input       
#         self.param.bo_diff += do_input
#         self.param.bg_diff += dg_input       

#         # compute bottom diff
#         dxc = np.zeros_like(self.xc)
#         dxc += np.dot(self.param.wi.T, di_input)
#         dxc += np.dot(self.param.wf.T, df_input)
#         dxc += np.dot(self.param.wo.T, do_input)
#         dxc += np.dot(self.param.wg.T, dg_input)

#         # save bottom diffs
#         self.state.bottom_diff_s = ds * self.state.f
#         self.state.bottom_diff_h = dxc[self.param.x_dim:]





            # compute the gradient of tanh with respect to state.values
            dtanh = self.activation.derivative(self.state.values[:, t]) * gradient

            # compute the gradient of the loss with respect to Wax (≈2 lines)
            dxt = np.dot(Wax.T, dtanh)
            dWax = np.dot(dtanh, xt.T)

            # compute the gradient with respect to Waa (≈2 lines)
            da_prev = np.dot(Waa.T, dtanh)
            dWaa = np.dot(dtanh, a_prev.T)

            # compute the gradient with respect to b (≈1 line)
            dba = np.sum(dtanh, 1, keepdims=True)

            ### END CODE HERE ###
            
            # ds = self.state.o * top_diff_h + top_diff_s
            # self.state.s = self.state.g * self.state.i + s_prev * self.state.f
            # self.state.h = self.state.s * self.state.o



                
            # # notice that top_diff_s is carried along the constant error carousel
            # ds = self.state.o * top_diff_h + top_diff_s
            # do = self.state.s * top_diff_h
            # di = self.state.g * ds
            # dg = self.state.i * ds
            # df = self.s_prev * ds

            # di_input = self.sigmoid.derivative(self.state.i) * di 
            # df_input = self.sigmoid.derivative(self.state.f) * df 
            # do_input = self.sigmoid.derivative(self.state.o) * do 
            # dg_input = self.activation.derivative(self.state.g) * dg


            # ft = self.sigmoid(xt.dot(Wf.T) + ct_prev.dot(Uf))
            # it = self.sigmoid(xt.dot(Wi.T) + ct_prev.dot(Ui))
            # ot = self.sigmoid(xt.dot(Wo.T) + ct_prev.dot(Uo))
            # ct_hat = self.activation(xt.dot(Wc.T) + ct_prev.dot(Uc))
            # ct = ft * ct_prev + it * self.activation(ct_hat)
            # st = ot * self.activation(ct)#(ht)/(at)

            # self.forget.values[:, t] = ft
            # self.input.values[:, t] = it
            # self.output.values[:, t] = ot 
            # self.cell.values[:, t] = ct
            # self.state.values[:, t] = st#(ht)/(at)

            # # next ht
            # outputs[:, t] = self.state.values[:, t].dot(self.state.W)
            
            # self.inputs[:, t] = X[:, t].dot(self.input_W.T) + self.states[:, t-1].dot(self.states_W.T)
            # self.states[:, t] = self.activation(self.inputs[:, t])
            # self.outputs[:, t] = self.states[:, t].dot(self.output_W.T)

            # gradient_state = self.activation.derivative(self.inputs[:, t]) * gradient[:, t].dot(self.output_W)
            # gradient_state = self.activation.derivative(self.inputs[:, t]) * gradient[:, t].dot(self.output_W)



        # # derivatives
        # self.reset.W_derivative = np.zeros_like(self.reset.W)
        # self.update.W_derivative = np.zeros_like(self.update.W)
        # self.hidden.W_derivative = np.zeros_like(self.hidden.W)

        # self.input.W_derivative = np.zeros_like(self.input.W)
        # # self.state.W_derivative = np.zeros_like(self.state.W)
        # self.output.W_derivative = np.zeros_like(self.output.W)

        # backward derivative (chain-rule)
        # gradient = loss * self.activation_output.derivative(self.output.values)# not been used 
        # gradient_next = np.zeros_like(gradient)



        # # pass trough neuron
        # for t in range(timesteps):
        #     self.inputs[:, t] = X[:, t].dot(self.input_W.T) + self.states[:, t-1].dot(self.states_W.T)
        #     self.states[:, t] = self.activation(self.inputs[:, t])
        #     self.outputs[:, t] = self.states[:, t].dot(self.output_W.T)


        # gradient_state = gradient[:, t].dot(self.output_W) * self.activation.derivative(self.inputs[:, t])
        # gradient_next[:, t] = gradient_state.dot(self.input_W)
            
        #     self.output_Wd += gradient[:, t].T.dot(self.states[:, t])
        #     for t_ in reversed(np.arange(max(0, t - self.bptt_trunc), t+1)):
        #         self.input_Wd += gradient_state.T.dot(self.layer_input[:, t_])
        #         self.states_Wd += gradient_state.T.dot(self.states[:, t_-1])
                
        #         gradient_state = gradient_state.dot(self.states_W) * self.activation.derivative(self.inputs[:, t_-1])

        # # update layer weights/params
        # _, timesteps, _ = gradient.shape
        # for t in reversed(range(timesteps)):
        #     gradient_state = gradient[:, t].dot(self.output.W) * self.activation.derivative(self.update.values[:, t])
        #     gradient_next[:, t] = gradient_state.dot(self.update.W)
        #     # gradient_next[:, t] = gradient_state.dot(self.input.W)
            
        #     self.reset.W_derivative += gradient[:, t].T.dot(self.state.values[:, t])
        #     self.update.W_derivative += gradient_state.T.dot(self.layer_input[:, t])
        #     self.hidden.W_derivative += gradient_state.T.dot(self.state.values[:, t-1])

        #     self.output.W_derivative += gradient[:, t].T.dot(self.state.values[:, t])
        #     # self.input.W_derivative += gradient_state.T.dot(self.layer_input[:, t])
        #     self.state.W_derivative += gradient_state.T.dot(self.state.values[:, t-1])

        #     gradient_state = gradient[:, t].dot(self.output.W) * self.activation.derivative(self.update.values[:, t])
        #     # gradient_next[:, t] = gradient_state.dot(self.input.W)

        #     # self.output.W_derivative += gradient[:, t].T.dot(self.state.values[:, t])
        #     # self.input.W_derivative += gradient_state.T.dot(self.layer_input[:, t])
        #     # self.state.W_derivative += gradient_state.T.dot(self.state.values[:, t-1])

        # self.reset.W  = self.reset.W_optimizer.update(self.reset.W, self.reset.W_derivative)
        # self.update.W = self.update.W_optimizer.update(self.update.W, self.update.W_derivative)
        # self.hidden.W = self.hidden.W_optimizer.update(self.hidden.W, self.hidden.W_derivative)

        # # self.input.W  = self.input.W_optimizer.update(self.input.W, self.input.W_derivative)
        # self.state.W = self.state.W_optimizer.update(self.state.W, self.state.W_derivative)
        # self.output.W = self.output.W_optimizer.update(self.output.W, self.output.W_derivative)

        # # Return gradient for next layer
        # # Calculated based on the weights used during the forward pass
        # return gradient_next


    # def __init__(self, n_units, input_shape=None, activation='tanh', activation_output="softmax", optimizer="adagrad"):
    #     super().__init__(n_units, input_shape, activation, optimizer)
    #     self.activation_output = act_functions[activation_output]()
    #     self.sigmoid = act_functions["sigmoid"]()

    #     self.forget   = Gate(self.optimizer)
    #     self.input    = Gate(self.optimizer)
    #     self.cell     = Gate(self.optimizer)
    #     self.cell_bar = Gate(self.optimizer)
    #     self.output   = Gate(self.optimizer)
    #     self.state    = Gate(self.optimizer)

    #     if self.input_shape != None:
    #         self.__call__(self.input_shape)

    # def __call__(self, input_shape=None):
    #     if self.input_shape is None:
    #         self.input_shape = input_shape
        
    #     input_dim = self.input_shape[0]
    #     hidden_dim = self.input_shape[1]
    #     total_dim = input_dim + hidden_dim

    #     # initialize gates
    #     self.forget.initialize(shape=(hidden_dim, total_dim))
    #     self.input.initialize(shape=(hidden_dim, total_dim))
    #     self.cell.initialize(shape=(hidden_dim, total_dim))
    #     self.cell_bar.initialize(shape=(hidden_dim, total_dim))
    #     self.output.initialize(shape=(hidden_dim, total_dim))
    #     self.state.initialize(shape=(input_dim, hidden_dim))

    # def forward(self, X):
        
    #     # prepare for backprop
    #     self.layer_input = X
    #     timesteps, _,  _ = X.shape
    #     y_pred = np.zeros_like(X)

    #     # Forward pass, through time steps
    #     for t in range(timesteps):
    #         z = np.row_stack((self.state.values[t - 1], X[t]))

    #         self.forget.values[t] = self.sigmoid(np.dot(self.forget.W, z) + self.forget.b)
    #         self.input.values[t] = self.sigmoid(np.dot(self.input.W, z) + self.input.b)
    #         self.output.values[t] = self.sigmoid(np.dot(self.output.W, z) + self.output.b)

    #         self.cell_bar.values[t] = self.activation(np.dot(self.cell.W, z) + self.cell.b)
    #         self.cell.values[t] = self.forget.values[t] * self.cell.values[t - 1] + self.input.values[t] * self.cell_bar.values[t]
            
    #         self.state.values[t] = self.output.values[t] * self.activation(self.cell.values[t])
    #         v = np.dot(self.state.W, self.state.values[t]) + self.state.b
            
    #         y_pred[t] = self.activation_output(v)#softmax

    #     return y_pred

    # def backward(self, loss): 
    #     timesteps, _,  _ = self.layer_input.shape
    #     self.forget.clear_derivatives()
    #     self.input.clear_derivatives()
    #     self.cell.clear_derivatives()
    #     self.output.clear_derivatives()
    #     self.state.clear_derivatives()
        
    #     #dh from the next character
    #     dh_next = np.zeros_like(self.state.values[0])

    #     #dC from the next character
    #     dC_next = np.zeros_like(self.cell.values[0])

    #     # Backward pass
    #     for t in reversed(range(timesteps)):
    #         z = np.row_stack((self.state.values[t - 1], self.layer_input[t]))
    #         loss_derivative = loss[t]

    #         self.state.b_derivative += loss_derivative
    #         self.state.W_derivative += np.dot(loss_derivative, self.state.values[t].T)

    #         dh = np.dot(self.state.W.T, loss_derivative) + dh_next 
    #         do = dh * self.activation(self.cell.values[t])
    #         do = self.sigmoid.derivative(self.output.values[t]) * do
    #         self.output.b_derivative += do
    #         self.output.W_derivative += np.dot(do, z.T)

    #         dC = np.copy(dC_next)
    #         dC += dh * self.output.values[t] * self.activation.derivative(self.activation(self.cell.values[t]))
    #         dC_bar = dC * self.input.values[t]
    #         dC_bar = self.activation.derivative(self.cell_bar.values[t]) * dC_bar
    #         self.cell.b_derivative += dC_bar
    #         self.cell.W_derivative += np.dot(dC_bar, z.T)

    #         di = dC * self.cell_bar.values[t]
    #         di = self.sigmoid.derivative(self.input.values[t]) * di
    #         self.input.b_derivative += di
    #         self.input.W_derivative += np.dot(di, z.T)

    #         df = dC * self.cell.values[t-1]
    #         df = self.sigmoid.derivative(self.forget.values[t]) * df
    #         self.forget.b_derivative += df
    #         self.forget.W_derivative += np.dot(df, z.T)

    #         dz = (np.dot(self.forget.W.T, df) + np.dot(self.input.W.T, di) + np.dot(self.cell.W.T, dC_bar) + np.dot(self.output.W.T, do))
    #         dh_next = dz[:self.input_shape[1], :]
    #         dC_next = self.forget.values[t] * dC

    #     self.forget.update_weights()
    #     self.input.update_weights()
    #     self.cell.update_weights()
    #     self.output.update_weights()
    #     self.state.update_weights()

    #     return loss


    # # # need for backprop
    # # self.layer_input = X

    # # # batch_size, timesteps, input_dim = X.shape
    # # # self.forgets = np.zeros((batch_size, timesteps, self.n_units))
    # # # self.inputs = np.zeros((batch_size, timesteps, self.n_units))
    # # # self.cells = np.zeros((batch_size, timesteps, self.n_units))
    # # # self.outputs = np.zeros((batch_size, timesteps, input_dim))
    # # # self.states = np.zeros((batch_size, timesteps+1, self.n_units))
    
    # # # # pass trough neuron
    # # # for t in range(timesteps):
    # # #     self.forgets[:, t] = self.activation_gates(
    # # #         X[:, t].dot(self.forget_W.T) + self.states[:, t-1].dot(self.forget_W.T)
    # # #     )
    # # #     self.inputs[:, t] = self.activation_gates(
    # # #         X[:, t].dot(self.input_W.T) + self.states[:, t-1].dot(self.input_W.T)
    # # #     )
    # # #     self.outputs[:, t] = self.activation_gates(
    # # #         X[:, t].dot(self.output_W.T) + self.states[:, t-1].dot(self.output_W.T)
    # # #     )
    # # #     self.cells[:, t] = self.forgets[:, t] * self.cells[:, t-1] + self.inputs[:, t] * self.activation_gates(
    # # #         X[:, t].dot(self.cell_W.T) + self.states[:, t-1].dot(self.cell_W.T)
    # # #     )
    # # #     self.states[:, t] = self.outputs[:, t] * self.activation_gates(self.cells[:, t])


    # #     # self.inputs[:, t] = X[:, t].dot(self.input_W.T) + self.states[:, t-1].dot(self.states_W.T)
    # #     # self.states[:, t] = self.activation(self.inputs[:, t])
    # #     # self.outputs[:, t] = self.states[:, t].dot(self.output_W.T)

    # # return self.activation_output(self.outputs)

    # # def backward(self, loss):
    # #     # # derivatives
    # #     # self.cell_Wd  = np.zeros_like(self.cell_W)
    # #     # self.forget_Wd  = np.zeros_like(self.forget_W)
    # #     # self.input_Wd  = np.zeros_like(self.input_W)
    # #     # self.states_Wd = np.zeros_like(self.states_W)
    # #     # self.output_Wd = np.zeros_like(self.output_W)

    # #     # # backward derivative (chain-rule)
    # #     # gradient = loss * self.activation_output.derivative(self.outputs)
    # #     # gradient_next = np.zeros_like(gradient)

    # #     # # update layer weights/params
    # #     # _, timesteps, _ = gradient.shape
    # #     # for t in reversed(range(timesteps)):
    # #     #     gradient_state = gradient[:, t].dot(self.output_W) * self.activation.derivative(self.inputs[:, t])
    # #     #     gradient_next[:, t] = gradient_state.dot(self.input_W)

    # #     #     self.output_Wd += gradient[:, t].T.dot(self.states[:, t])
    # #     #     for t_ in reversed(np.arange(max(0, t - self.bptt_trunc), t+1)):
    # #     #         self.input_Wd += gradient_state.T.dot(self.layer_input[:, t_])
    # #     #         self.states_Wd += gradient_state.T.dot(self.states[:, t_-1])
                
    # #     #         gradient_state = gradient_state.dot(self.states_W) * self.activation.derivative(self.inputs[:, t_-1])

    # #     # self.input_W  = self.input_W_optimizer.update(self.input_W, self.input_Wd)
    # #     # self.states_W = self.states_W_optimizer.update(self.states_W, self.states_Wd)
    # #     # self.output_W = self.output_W_optimizer.update(self.output_W, self.output_Wd)

    # #     # Return gradient for next layer
    # #     # Calculated based on the weights used during the forward pass
    # #     # return gradient_next
    # #     return loss